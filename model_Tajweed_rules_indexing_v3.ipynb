{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP8/4FADxVvairGa0vguhSX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aWO1vAVp7HkG","executionInfo":{"status":"ok","timestamp":1715613114389,"user_tz":-60,"elapsed":3458,"user":{"displayName":"Safa BENABDESSADOK","userId":"13084690164500108428"}},"outputId":"baa14372-b247-41d5-889c-93e21f812c4a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import os\n","import tensorflow as tf\n","from tensorflow import keras\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from tensorflow.keras.layers import Input, Flatten, Dense\n","from tensorflow.keras.models import Model"],"metadata":{"id":"6I2s5Q0iDpDE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load data\n","data = pd.read_csv('/content/drive/My Drive/M2 GL/PFE/Data/hisb_60_and_Al_fatihah_audio_with_transcript_and_MFCC_and_ahkam_indexing_v2.csv')"],"metadata":{"id":"WtJVQemz7klg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["export_dir = '/content/drive/My Drive/M2 GL/PFE/AI_models_v3'"],"metadata":{"id":"7A9PSizPHQos"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["abdul_basit = data[data['recitor_en'] == 'Abdul Basit']\n","yassin_aljazaery = data[data['recitor_en'] == 'Yassin Al Jazaery']\n","ibrahim_aldosary = data[data['recitor_en'] == 'Ibrahim_Aldosary']"],"metadata":{"id":"KoKbTBQr7nY8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["splitted_data_info_np = np.empty((0, 6))\n","models_information_np = np.empty((0, 5))"],"metadata":{"id":"_CenkKQf-AXc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def max_sequence_length_X_Y(data, tajweed_rule):\n","  data_filtered = data[data[tajweed_rule].apply(lambda x: x != '[-1]')]\n","  X_raw = data_filtered['mfcc'].astype(str).tolist()\n","  Y_raw = data_filtered[tajweed_rule].astype(str).tolist()\n","  X = [tf.constant(eval(x)) for x in X_raw]\n","  Y = [tf.constant(eval(x)) for x in Y_raw]\n","  max_sequence_length_Y = max(len(seq) for seq in Y)\n","  max_sequence_length_X = max(len(seq) for seq in X)\n","  return max_sequence_length_X, max_sequence_length_Y"],"metadata":{"id":"H950tSVTMxlL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def data_preparation(reciter_data, tajweed_rule, max_X, max_Y):\n","  data_filtered = reciter_data[reciter_data[tajweed_rule].apply(lambda x: x != '[-1]')]\n","\n","  # Extract 'mfcc' and tajweed_rule columns as lists of strings\n","  X_raw = data_filtered['mfcc'].astype(str).tolist()\n","  Y_raw = data_filtered[tajweed_rule].astype(str).tolist()\n","\n","  # Preprocess the input data\n","  X = [tf.constant(eval(x)) for x in X_raw]\n","  Y = [tf.constant(eval(x)) for x in Y_raw]\n","\n","  # Pad sequences in Y and in X to ensure all have the same length\n","  Y_padded = tf.keras.preprocessing.sequence.pad_sequences(Y, maxlen=max_Y, padding='post', dtype='int32', value=-1)\n","  X_padded = tf.keras.preprocessing.sequence.pad_sequences(X, maxlen=max_X, padding='post', dtype='float32')\n","\n","  # Split the data into training and testing sets\n","  X_train, X_test, Y_train, Y_test = train_test_split(X_padded, Y_padded, test_size=0.2, random_state=10)\n","  return X_train, X_test, Y_train, Y_test"],"metadata":{"id":"pVbn5z76-K4O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def tajweed_rule_model(reciter1, reciter2, reciter3, tajweed_rule):\n","  global splitted_data_info_np, models_information_np, data\n","\n","  max_X, max_Y = max_sequence_length_X_Y(data, tajweed_rule)\n","\n","  # data preparation\n","  reciter1_X_train, reciter1_X_test, reciter1_Y_train, reciter1_Y_test = data_preparation(reciter1, tajweed_rule, max_X, max_Y)\n","  reciter2_X_train, reciter2_X_test, reciter2_Y_train, reciter2_Y_test = data_preparation(reciter2, tajweed_rule, max_X, max_Y)\n","  reciter3_X_train, reciter3_X_test, reciter3_Y_train, reciter3_Y_test = data_preparation(reciter3, tajweed_rule, max_X, max_Y)\n","\n","  # Update splitted_data_info with information about each reciter\n","  for reciter_X_train, reciter_X_test, reciter_Y_train, reciter_Y_test, reciter_data in [\n","      (reciter1_X_train, reciter1_X_test, reciter1_Y_train, reciter1_Y_test, reciter1),\n","      (reciter2_X_train, reciter2_X_test, reciter2_Y_train, reciter2_Y_test, reciter2),\n","      (reciter3_X_train, reciter3_X_test, reciter3_Y_train, reciter3_Y_test, reciter3)]:\n","\n","      splitted_data_info_np = np.append(splitted_data_info_np, [[\n","              tajweed_rule,\n","              reciter_data.iloc[0]['recitor_en'],\n","              len(reciter_X_train),\n","              len(reciter_X_test),\n","              len(reciter_Y_train),\n","              len(reciter_Y_test)\n","              ]], axis=0)\n","\n","  # concatenate data\n","  # training data\n","  X_train = np.concatenate([reciter1_X_train, reciter2_X_train, reciter3_X_train], axis=0)\n","  Y_train = np.concatenate([reciter1_Y_train, reciter2_Y_train, reciter3_Y_train], axis=0)\n","\n","  # testing data\n","  X_test = np.concatenate([reciter1_X_test, reciter2_X_test, reciter3_X_test], axis=0)\n","  Y_test = np.concatenate([reciter1_Y_test, reciter2_Y_test, reciter3_Y_test], axis=0)\n","\n","  splitted_data_info_np = np.append(splitted_data_info_np, [[\n","          tajweed_rule,\n","          'all reciters',\n","          len(X_train),\n","          len(X_test),\n","          len(Y_train),\n","          len(Y_test)\n","          ]], axis=0)\n","\n","  # Normalize input data by scaling each sequence individually\n","  scaler = StandardScaler()\n","  X_train_scaled = np.array([scaler.fit_transform(seq) for seq in X_train])\n","  X_test_scaled = np.array([scaler.transform(seq) for seq in X_test])\n","\n","  # Define a simple neural network model\n","  input_shape = X_train_scaled[0].shape  # Shape of each mfcc sequence\n","  output_shape = Y_train.shape[1]  # Dimension of output (number of units in output layer)\n","\n","  input_layer = Input(shape=input_shape)\n","  flatten_layer = Flatten()(input_layer)  # Flatten the sequence to a 1D vector\n","  hidden_layer = Dense(64, activation='relu')(flatten_layer)\n","  output_layer = Dense(output_shape, activation='linear')(hidden_layer)  # Define the output layer with the correct units\n","\n","  model = Model(inputs=input_layer, outputs=output_layer)\n","\n","  # Compile the model\n","  model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n","\n","  # Train the model\n","  model.fit(X_train_scaled, Y_train, epochs=50, batch_size=32, validation_split=0.1)\n","\n","  #export the model\n","  model_filename = f'{tajweed_rule}_tajweed_rule_model'\n","  model_path = os.path.join(export_dir, model_filename)\n","  keras.models.save_model(model, model_path)\n","\n","  # Make predictions on test data\n","  predictions = model.predict(X_test_scaled)\n","\n","  # Evaluate the model with adjusted predictions\n","  predictions[predictions < 0] = -1\n","  predictions = np.round(predictions).astype('int32')\n","  loss, accuracy = model.evaluate(X_test_scaled, predictions)\n","\n","  print(f\"Test Loss: {loss:.4f}, Test accuracy : {accuracy:.4f}\")\n","  models_information_np = np.append(models_information_np, [[\n","          model_filename,\n","          \"{:.4f}\".format(loss),\n","          \"{:.4f}\".format(accuracy),\n","          \"{:.2f}\".format(accuracy*100),\n","          model_path]], axis=0)"],"metadata":{"id":"n3ycP8uz8zp8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tajweed_rules = ['madd_6_Lazim', 'madd_246', 'madd_6', 'madd_2', 'Ikhfaa', 'Idgham', 'tafkhim', 'qalqala', 'imala']"],"metadata":{"id":"qpgV2OY8K09J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for rule in tajweed_rules:\n","  tajweed_rule_model(abdul_basit, yassin_aljazaery, ibrahim_aldosary, rule)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n6qxa6hWLAeZ","executionInfo":{"status":"ok","timestamp":1715613868750,"user_tz":-60,"elapsed":740452,"user":{"displayName":"Safa BENABDESSADOK","userId":"13084690164500108428"}},"outputId":"7b69897f-0a46-45bb-fd1c-29e883532841"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","1/1 [==============================] - 1s 1s/step - loss: 177.7901 - accuracy: 1.0000 - val_loss: 2.6397 - val_accuracy: 1.0000\n","Epoch 2/50\n","1/1 [==============================] - 0s 53ms/step - loss: 333.1300 - accuracy: 0.5000 - val_loss: 26.4664 - val_accuracy: 1.0000\n","Epoch 3/50\n","1/1 [==============================] - 0s 53ms/step - loss: 282.1169 - accuracy: 0.5000 - val_loss: 56.0670 - val_accuracy: 1.0000\n","Epoch 4/50\n","1/1 [==============================] - 0s 52ms/step - loss: 27.2498 - accuracy: 0.5000 - val_loss: 94.9248 - val_accuracy: 1.0000\n","Epoch 5/50\n","1/1 [==============================] - 0s 52ms/step - loss: 73.8831 - accuracy: 0.5000 - val_loss: 118.5578 - val_accuracy: 0.0000e+00\n","Epoch 6/50\n","1/1 [==============================] - 0s 56ms/step - loss: 144.3461 - accuracy: 0.5000 - val_loss: 113.4313 - val_accuracy: 1.0000\n","Epoch 7/50\n","1/1 [==============================] - 0s 69ms/step - loss: 115.4410 - accuracy: 0.5000 - val_loss: 92.0112 - val_accuracy: 1.0000\n","Epoch 8/50\n","1/1 [==============================] - 0s 52ms/step - loss: 83.1981 - accuracy: 0.5000 - val_loss: 65.4250 - val_accuracy: 1.0000\n","Epoch 9/50\n","1/1 [==============================] - 0s 54ms/step - loss: 53.6063 - accuracy: 0.5000 - val_loss: 43.3746 - val_accuracy: 1.0000\n","Epoch 10/50\n","1/1 [==============================] - 0s 72ms/step - loss: 32.9828 - accuracy: 0.5000 - val_loss: 30.2110 - val_accuracy: 1.0000\n","Epoch 11/50\n","1/1 [==============================] - 0s 59ms/step - loss: 33.3339 - accuracy: 1.0000 - val_loss: 26.4012 - val_accuracy: 1.0000\n","Epoch 12/50\n","1/1 [==============================] - 0s 73ms/step - loss: 41.8302 - accuracy: 1.0000 - val_loss: 30.7278 - val_accuracy: 1.0000\n","Epoch 13/50\n","1/1 [==============================] - 0s 70ms/step - loss: 39.2822 - accuracy: 0.5000 - val_loss: 42.2747 - val_accuracy: 0.0000e+00\n","Epoch 14/50\n","1/1 [==============================] - 0s 55ms/step - loss: 27.1907 - accuracy: 0.5000 - val_loss: 59.1715 - val_accuracy: 1.0000\n","Epoch 15/50\n","1/1 [==============================] - 0s 54ms/step - loss: 21.0986 - accuracy: 0.0000e+00 - val_loss: 77.5782 - val_accuracy: 1.0000\n","Epoch 16/50\n","1/1 [==============================] - 0s 55ms/step - loss: 28.4309 - accuracy: 0.5000 - val_loss: 90.9158 - val_accuracy: 1.0000\n","Epoch 17/50\n","1/1 [==============================] - 0s 53ms/step - loss: 38.7348 - accuracy: 0.5000 - val_loss: 94.3120 - val_accuracy: 1.0000\n","Epoch 18/50\n","1/1 [==============================] - 0s 61ms/step - loss: 35.9152 - accuracy: 1.0000 - val_loss: 87.6233 - val_accuracy: 1.0000\n","Epoch 19/50\n","1/1 [==============================] - 0s 53ms/step - loss: 19.7806 - accuracy: 1.0000 - val_loss: 74.9381 - val_accuracy: 1.0000\n","Epoch 20/50\n","1/1 [==============================] - 0s 53ms/step - loss: 5.7219 - accuracy: 0.5000 - val_loss: 61.2786 - val_accuracy: 0.0000e+00\n","Epoch 21/50\n","1/1 [==============================] - 0s 69ms/step - loss: 3.7044 - accuracy: 0.5000 - val_loss: 50.2278 - val_accuracy: 0.0000e+00\n","Epoch 22/50\n","1/1 [==============================] - 0s 73ms/step - loss: 8.9338 - accuracy: 0.5000 - val_loss: 43.6915 - val_accuracy: 1.0000\n","Epoch 23/50\n","1/1 [==============================] - 0s 53ms/step - loss: 14.1183 - accuracy: 0.5000 - val_loss: 42.3103 - val_accuracy: 1.0000\n","Epoch 24/50\n","1/1 [==============================] - 0s 71ms/step - loss: 17.3875 - accuracy: 1.0000 - val_loss: 45.5966 - val_accuracy: 1.0000\n","Epoch 25/50\n","1/1 [==============================] - 0s 55ms/step - loss: 17.4660 - accuracy: 1.0000 - val_loss: 52.2749 - val_accuracy: 1.0000\n","Epoch 26/50\n","1/1 [==============================] - 0s 71ms/step - loss: 12.7023 - accuracy: 0.5000 - val_loss: 60.7140 - val_accuracy: 1.0000\n","Epoch 27/50\n","1/1 [==============================] - 0s 70ms/step - loss: 6.2841 - accuracy: 0.5000 - val_loss: 69.0201 - val_accuracy: 1.0000\n","Epoch 28/50\n","1/1 [==============================] - 0s 53ms/step - loss: 3.6450 - accuracy: 0.5000 - val_loss: 75.0945 - val_accuracy: 1.0000\n","Epoch 29/50\n","1/1 [==============================] - 0s 53ms/step - loss: 5.4664 - accuracy: 0.5000 - val_loss: 77.2898 - val_accuracy: 1.0000\n","Epoch 30/50\n","1/1 [==============================] - 0s 61ms/step - loss: 7.3655 - accuracy: 0.5000 - val_loss: 75.2522 - val_accuracy: 1.0000\n","Epoch 31/50\n","1/1 [==============================] - 0s 56ms/step - loss: 6.0878 - accuracy: 0.5000 - val_loss: 70.0519 - val_accuracy: 1.0000\n","Epoch 32/50\n","1/1 [==============================] - 0s 69ms/step - loss: 3.4572 - accuracy: 1.0000 - val_loss: 63.4906 - val_accuracy: 1.0000\n","Epoch 33/50\n","1/1 [==============================] - 0s 51ms/step - loss: 3.2177 - accuracy: 1.0000 - val_loss: 57.3745 - val_accuracy: 1.0000\n","Epoch 34/50\n","1/1 [==============================] - 0s 71ms/step - loss: 5.2493 - accuracy: 0.5000 - val_loss: 53.1137 - val_accuracy: 1.0000\n","Epoch 35/50\n","1/1 [==============================] - 0s 70ms/step - loss: 6.1934 - accuracy: 0.5000 - val_loss: 51.5857 - val_accuracy: 1.0000\n","Epoch 36/50\n","1/1 [==============================] - 0s 70ms/step - loss: 4.9283 - accuracy: 1.0000 - val_loss: 53.0582 - val_accuracy: 1.0000\n","Epoch 37/50\n","1/1 [==============================] - 0s 53ms/step - loss: 3.0438 - accuracy: 1.0000 - val_loss: 57.1066 - val_accuracy: 1.0000\n","Epoch 38/50\n","1/1 [==============================] - 0s 70ms/step - loss: 1.5052 - accuracy: 0.5000 - val_loss: 62.5999 - val_accuracy: 1.0000\n","Epoch 39/50\n","1/1 [==============================] - 0s 59ms/step - loss: 0.6511 - accuracy: 0.5000 - val_loss: 67.8934 - val_accuracy: 1.0000\n","Epoch 40/50\n","1/1 [==============================] - 0s 56ms/step - loss: 1.1914 - accuracy: 0.5000 - val_loss: 71.2913 - val_accuracy: 1.0000\n","Epoch 41/50\n","1/1 [==============================] - 0s 58ms/step - loss: 2.7104 - accuracy: 1.0000 - val_loss: 71.6859 - val_accuracy: 1.0000\n","Epoch 42/50\n","1/1 [==============================] - 0s 63ms/step - loss: 3.4357 - accuracy: 1.0000 - val_loss: 69.0859 - val_accuracy: 1.0000\n","Epoch 43/50\n","1/1 [==============================] - 0s 69ms/step - loss: 2.5372 - accuracy: 1.0000 - val_loss: 64.6473 - val_accuracy: 1.0000\n","Epoch 44/50\n","1/1 [==============================] - 0s 69ms/step - loss: 1.1783 - accuracy: 1.0000 - val_loss: 60.0858 - val_accuracy: 1.0000\n","Epoch 45/50\n","1/1 [==============================] - 0s 71ms/step - loss: 0.8231 - accuracy: 1.0000 - val_loss: 56.8833 - val_accuracy: 1.0000\n","Epoch 46/50\n","1/1 [==============================] - 0s 53ms/step - loss: 1.2437 - accuracy: 0.5000 - val_loss: 55.8481 - val_accuracy: 1.0000\n","Epoch 47/50\n","1/1 [==============================] - 0s 54ms/step - loss: 1.2509 - accuracy: 1.0000 - val_loss: 57.0639 - val_accuracy: 1.0000\n","Epoch 48/50\n","1/1 [==============================] - 0s 62ms/step - loss: 0.8120 - accuracy: 1.0000 - val_loss: 59.9419 - val_accuracy: 1.0000\n","Epoch 49/50\n","1/1 [==============================] - 0s 56ms/step - loss: 0.6533 - accuracy: 1.0000 - val_loss: 63.3916 - val_accuracy: 1.0000\n","Epoch 50/50\n","1/1 [==============================] - 0s 70ms/step - loss: 0.7734 - accuracy: 1.0000 - val_loss: 66.2346 - val_accuracy: 1.0000\n","1/1 [==============================] - 0s 55ms/step\n","1/1 [==============================] - 0s 27ms/step - loss: 112.4650 - accuracy: 0.6667\n","Test Loss: 112.4650, Test accuracy : 0.6667\n","Epoch 1/50\n","6/6 [==============================] - 1s 77ms/step - loss: 255.7089 - accuracy: 0.7605 - val_loss: 227.3584 - val_accuracy: 0.8947\n","Epoch 2/50\n","6/6 [==============================] - 0s 45ms/step - loss: 119.5778 - accuracy: 0.8982 - val_loss: 230.0931 - val_accuracy: 0.8947\n","Epoch 3/50\n","6/6 [==============================] - 0s 49ms/step - loss: 83.1841 - accuracy: 0.8982 - val_loss: 211.0047 - val_accuracy: 0.8947\n","Epoch 4/50\n","6/6 [==============================] - 0s 41ms/step - loss: 58.2902 - accuracy: 0.8982 - val_loss: 163.5929 - val_accuracy: 0.8947\n","Epoch 5/50\n","6/6 [==============================] - 0s 41ms/step - loss: 40.3559 - accuracy: 0.9222 - val_loss: 170.5374 - val_accuracy: 0.8947\n","Epoch 6/50\n","6/6 [==============================] - 0s 44ms/step - loss: 24.8744 - accuracy: 0.9401 - val_loss: 176.1756 - val_accuracy: 0.8947\n","Epoch 7/50\n","6/6 [==============================] - 0s 45ms/step - loss: 20.4809 - accuracy: 0.9461 - val_loss: 177.2739 - val_accuracy: 0.8947\n","Epoch 8/50\n","6/6 [==============================] - 0s 41ms/step - loss: 12.5287 - accuracy: 0.9581 - val_loss: 171.3540 - val_accuracy: 0.8947\n","Epoch 9/50\n","6/6 [==============================] - 0s 40ms/step - loss: 9.0750 - accuracy: 0.9701 - val_loss: 173.7712 - val_accuracy: 0.8947\n","Epoch 10/50\n","6/6 [==============================] - 0s 44ms/step - loss: 6.9849 - accuracy: 0.9701 - val_loss: 169.6639 - val_accuracy: 0.8947\n","Epoch 11/50\n","6/6 [==============================] - 0s 47ms/step - loss: 5.4511 - accuracy: 0.9521 - val_loss: 172.8831 - val_accuracy: 0.8947\n","Epoch 12/50\n","6/6 [==============================] - 0s 42ms/step - loss: 3.8868 - accuracy: 0.9701 - val_loss: 172.3031 - val_accuracy: 0.8947\n","Epoch 13/50\n","6/6 [==============================] - 0s 43ms/step - loss: 3.0669 - accuracy: 0.9701 - val_loss: 166.7400 - val_accuracy: 0.8947\n","Epoch 14/50\n","6/6 [==============================] - 0s 43ms/step - loss: 2.1458 - accuracy: 0.9820 - val_loss: 168.0181 - val_accuracy: 0.8947\n","Epoch 15/50\n","6/6 [==============================] - 0s 52ms/step - loss: 1.6644 - accuracy: 0.9701 - val_loss: 174.5543 - val_accuracy: 0.8947\n","Epoch 16/50\n","6/6 [==============================] - 0s 55ms/step - loss: 2.0447 - accuracy: 0.9641 - val_loss: 172.8595 - val_accuracy: 0.8947\n","Epoch 17/50\n","6/6 [==============================] - 0s 63ms/step - loss: 1.9614 - accuracy: 0.9701 - val_loss: 170.6344 - val_accuracy: 0.8947\n","Epoch 18/50\n","6/6 [==============================] - 0s 63ms/step - loss: 1.6359 - accuracy: 0.9760 - val_loss: 168.4156 - val_accuracy: 0.8947\n","Epoch 19/50\n","6/6 [==============================] - 0s 55ms/step - loss: 1.7816 - accuracy: 0.9701 - val_loss: 169.2253 - val_accuracy: 0.8947\n","Epoch 20/50\n","6/6 [==============================] - 0s 63ms/step - loss: 1.4070 - accuracy: 0.9880 - val_loss: 173.5203 - val_accuracy: 0.8947\n","Epoch 21/50\n","6/6 [==============================] - 0s 67ms/step - loss: 2.6024 - accuracy: 0.9701 - val_loss: 171.1874 - val_accuracy: 0.8947\n","Epoch 22/50\n","6/6 [==============================] - 0s 71ms/step - loss: 3.8804 - accuracy: 0.9760 - val_loss: 169.4912 - val_accuracy: 0.8947\n","Epoch 23/50\n","6/6 [==============================] - 0s 65ms/step - loss: 5.7694 - accuracy: 0.9820 - val_loss: 169.4342 - val_accuracy: 0.8947\n","Epoch 24/50\n","6/6 [==============================] - 0s 54ms/step - loss: 3.4553 - accuracy: 0.9760 - val_loss: 176.3235 - val_accuracy: 0.8947\n","Epoch 25/50\n","6/6 [==============================] - 0s 73ms/step - loss: 4.1645 - accuracy: 0.9701 - val_loss: 171.7455 - val_accuracy: 0.8947\n","Epoch 26/50\n","6/6 [==============================] - 0s 73ms/step - loss: 3.4360 - accuracy: 0.9940 - val_loss: 170.3617 - val_accuracy: 0.8947\n","Epoch 27/50\n","6/6 [==============================] - 0s 68ms/step - loss: 2.6807 - accuracy: 0.9341 - val_loss: 173.8004 - val_accuracy: 0.8947\n","Epoch 28/50\n","6/6 [==============================] - 0s 63ms/step - loss: 2.4465 - accuracy: 0.9760 - val_loss: 170.9644 - val_accuracy: 0.8947\n","Epoch 29/50\n","6/6 [==============================] - 0s 72ms/step - loss: 2.4084 - accuracy: 0.9820 - val_loss: 175.3309 - val_accuracy: 0.8947\n","Epoch 30/50\n","6/6 [==============================] - 0s 70ms/step - loss: 2.7458 - accuracy: 0.9701 - val_loss: 166.3241 - val_accuracy: 0.8947\n","Epoch 31/50\n","6/6 [==============================] - 0s 61ms/step - loss: 2.3289 - accuracy: 0.9880 - val_loss: 172.8611 - val_accuracy: 0.8947\n","Epoch 32/50\n","6/6 [==============================] - 0s 80ms/step - loss: 2.2810 - accuracy: 0.9760 - val_loss: 169.6178 - val_accuracy: 0.8947\n","Epoch 33/50\n","6/6 [==============================] - 0s 65ms/step - loss: 1.9768 - accuracy: 0.9760 - val_loss: 172.6501 - val_accuracy: 0.8947\n","Epoch 34/50\n","6/6 [==============================] - 0s 67ms/step - loss: 1.6248 - accuracy: 0.9880 - val_loss: 171.5567 - val_accuracy: 0.8947\n","Epoch 35/50\n","6/6 [==============================] - 0s 66ms/step - loss: 1.7823 - accuracy: 0.9760 - val_loss: 168.8729 - val_accuracy: 0.8947\n","Epoch 36/50\n","6/6 [==============================] - 0s 69ms/step - loss: 1.6519 - accuracy: 0.9880 - val_loss: 168.0267 - val_accuracy: 0.8947\n","Epoch 37/50\n","6/6 [==============================] - 0s 67ms/step - loss: 1.8762 - accuracy: 0.9820 - val_loss: 169.3451 - val_accuracy: 0.8947\n","Epoch 38/50\n","6/6 [==============================] - 0s 59ms/step - loss: 2.0730 - accuracy: 0.9940 - val_loss: 170.1556 - val_accuracy: 0.8947\n","Epoch 39/50\n","6/6 [==============================] - 0s 64ms/step - loss: 27.2978 - accuracy: 0.9641 - val_loss: 173.7821 - val_accuracy: 0.8947\n","Epoch 40/50\n","6/6 [==============================] - 0s 63ms/step - loss: 9.4550 - accuracy: 0.9760 - val_loss: 174.7514 - val_accuracy: 0.8947\n","Epoch 41/50\n","6/6 [==============================] - 0s 51ms/step - loss: 10.3250 - accuracy: 0.9701 - val_loss: 171.4520 - val_accuracy: 0.8947\n","Epoch 42/50\n","6/6 [==============================] - 0s 40ms/step - loss: 8.8836 - accuracy: 0.9641 - val_loss: 162.9429 - val_accuracy: 0.8947\n","Epoch 43/50\n","6/6 [==============================] - 0s 42ms/step - loss: 6.2794 - accuracy: 0.9581 - val_loss: 164.5457 - val_accuracy: 0.8947\n","Epoch 44/50\n","6/6 [==============================] - 0s 43ms/step - loss: 4.8508 - accuracy: 0.9701 - val_loss: 171.2566 - val_accuracy: 0.8947\n","Epoch 45/50\n","6/6 [==============================] - 0s 41ms/step - loss: 5.8586 - accuracy: 0.9641 - val_loss: 174.2526 - val_accuracy: 0.8947\n","Epoch 46/50\n","6/6 [==============================] - 0s 41ms/step - loss: 4.2107 - accuracy: 0.9701 - val_loss: 173.9819 - val_accuracy: 0.8947\n","Epoch 47/50\n","6/6 [==============================] - 0s 44ms/step - loss: 4.2183 - accuracy: 0.9581 - val_loss: 170.7681 - val_accuracy: 0.8947\n","Epoch 48/50\n","6/6 [==============================] - 0s 47ms/step - loss: 4.1991 - accuracy: 0.9641 - val_loss: 168.5456 - val_accuracy: 0.8947\n","Epoch 49/50\n","6/6 [==============================] - 0s 41ms/step - loss: 5.8667 - accuracy: 0.9641 - val_loss: 168.0325 - val_accuracy: 0.8947\n","Epoch 50/50\n","6/6 [==============================] - 0s 43ms/step - loss: 5.5811 - accuracy: 0.9880 - val_loss: 168.2338 - val_accuracy: 0.8947\n","2/2 [==============================] - 0s 10ms/step\n","2/2 [==============================] - 0s 13ms/step - loss: 1.0305 - accuracy: 1.0000\n","Test Loss: 1.0305, Test accuracy : 1.0000\n","Epoch 1/50\n","4/4 [==============================] - 1s 154ms/step - loss: 402.8904 - accuracy: 0.2222 - val_loss: 477.5382 - val_accuracy: 0.7333\n","Epoch 2/50\n","4/4 [==============================] - 0s 94ms/step - loss: 183.9247 - accuracy: 0.7381 - val_loss: 343.3277 - val_accuracy: 0.6000\n","Epoch 3/50\n","4/4 [==============================] - 0s 93ms/step - loss: 103.9599 - accuracy: 0.4444 - val_loss: 255.9100 - val_accuracy: 0.6667\n","Epoch 4/50\n","4/4 [==============================] - 0s 84ms/step - loss: 74.4607 - accuracy: 0.5159 - val_loss: 259.3217 - val_accuracy: 0.4667\n","Epoch 5/50\n","4/4 [==============================] - 0s 85ms/step - loss: 55.2365 - accuracy: 0.4444 - val_loss: 260.6402 - val_accuracy: 0.2667\n","Epoch 6/50\n","4/4 [==============================] - 0s 96ms/step - loss: 46.1375 - accuracy: 0.4286 - val_loss: 247.5128 - val_accuracy: 0.4667\n","Epoch 7/50\n","4/4 [==============================] - 0s 84ms/step - loss: 33.7897 - accuracy: 0.3651 - val_loss: 249.8119 - val_accuracy: 0.7333\n","Epoch 8/50\n","4/4 [==============================] - 0s 90ms/step - loss: 28.3716 - accuracy: 0.4762 - val_loss: 245.4906 - val_accuracy: 0.6667\n","Epoch 9/50\n","4/4 [==============================] - 0s 88ms/step - loss: 24.1897 - accuracy: 0.5397 - val_loss: 249.4137 - val_accuracy: 0.4667\n","Epoch 10/50\n","4/4 [==============================] - 0s 97ms/step - loss: 19.6619 - accuracy: 0.6349 - val_loss: 251.9643 - val_accuracy: 0.7333\n","Epoch 11/50\n","4/4 [==============================] - 0s 83ms/step - loss: 17.2461 - accuracy: 0.7857 - val_loss: 256.1131 - val_accuracy: 0.7333\n","Epoch 12/50\n","4/4 [==============================] - 0s 90ms/step - loss: 15.4434 - accuracy: 0.7540 - val_loss: 250.2843 - val_accuracy: 0.7333\n","Epoch 13/50\n","4/4 [==============================] - 0s 87ms/step - loss: 13.7196 - accuracy: 0.7460 - val_loss: 246.7849 - val_accuracy: 0.6667\n","Epoch 14/50\n","4/4 [==============================] - 0s 114ms/step - loss: 11.3797 - accuracy: 0.7540 - val_loss: 245.7444 - val_accuracy: 0.6000\n","Epoch 15/50\n","4/4 [==============================] - 0s 126ms/step - loss: 9.9965 - accuracy: 0.7460 - val_loss: 244.9614 - val_accuracy: 0.6667\n","Epoch 16/50\n","4/4 [==============================] - 0s 118ms/step - loss: 8.7224 - accuracy: 0.7540 - val_loss: 244.3409 - val_accuracy: 0.7333\n","Epoch 17/50\n","4/4 [==============================] - 1s 265ms/step - loss: 8.5028 - accuracy: 0.7857 - val_loss: 240.1753 - val_accuracy: 0.5333\n","Epoch 18/50\n","4/4 [==============================] - 2s 495ms/step - loss: 7.0523 - accuracy: 0.7619 - val_loss: 238.9263 - val_accuracy: 0.5333\n","Epoch 19/50\n","4/4 [==============================] - 1s 267ms/step - loss: 6.1597 - accuracy: 0.7698 - val_loss: 236.7870 - val_accuracy: 0.6000\n","Epoch 20/50\n","4/4 [==============================] - 1s 210ms/step - loss: 5.8835 - accuracy: 0.7460 - val_loss: 235.5944 - val_accuracy: 0.4667\n","Epoch 21/50\n","4/4 [==============================] - 1s 281ms/step - loss: 5.2334 - accuracy: 0.7778 - val_loss: 234.4763 - val_accuracy: 0.4667\n","Epoch 22/50\n","4/4 [==============================] - 1s 208ms/step - loss: 4.6766 - accuracy: 0.7619 - val_loss: 230.8620 - val_accuracy: 0.5333\n","Epoch 23/50\n","4/4 [==============================] - 1s 146ms/step - loss: 4.2746 - accuracy: 0.7857 - val_loss: 230.7847 - val_accuracy: 0.4667\n","Epoch 24/50\n","4/4 [==============================] - 1s 182ms/step - loss: 4.5594 - accuracy: 0.7619 - val_loss: 228.9297 - val_accuracy: 0.7333\n","Epoch 25/50\n","4/4 [==============================] - 0s 95ms/step - loss: 4.8691 - accuracy: 0.7778 - val_loss: 233.0004 - val_accuracy: 0.4000\n","Epoch 26/50\n","4/4 [==============================] - 1s 172ms/step - loss: 4.0213 - accuracy: 0.7222 - val_loss: 226.7890 - val_accuracy: 0.6667\n","Epoch 27/50\n","4/4 [==============================] - 1s 179ms/step - loss: 3.5318 - accuracy: 0.7857 - val_loss: 228.0631 - val_accuracy: 0.4667\n","Epoch 28/50\n","4/4 [==============================] - 1s 191ms/step - loss: 3.4308 - accuracy: 0.7857 - val_loss: 226.3289 - val_accuracy: 0.5333\n","Epoch 29/50\n","4/4 [==============================] - 0s 115ms/step - loss: 3.1046 - accuracy: 0.7857 - val_loss: 225.5727 - val_accuracy: 0.6000\n","Epoch 30/50\n","4/4 [==============================] - 1s 140ms/step - loss: 2.8868 - accuracy: 0.8175 - val_loss: 224.9246 - val_accuracy: 0.5333\n","Epoch 31/50\n","4/4 [==============================] - 1s 143ms/step - loss: 2.6261 - accuracy: 0.8095 - val_loss: 223.8105 - val_accuracy: 0.6000\n","Epoch 32/50\n","4/4 [==============================] - 1s 118ms/step - loss: 2.8793 - accuracy: 0.7937 - val_loss: 224.2166 - val_accuracy: 0.6667\n","Epoch 33/50\n","4/4 [==============================] - 0s 100ms/step - loss: 2.3988 - accuracy: 0.8016 - val_loss: 224.2617 - val_accuracy: 0.6667\n","Epoch 34/50\n","4/4 [==============================] - 0s 90ms/step - loss: 3.2592 - accuracy: 0.8095 - val_loss: 225.0945 - val_accuracy: 0.5333\n","Epoch 35/50\n","4/4 [==============================] - 0s 84ms/step - loss: 3.6486 - accuracy: 0.7937 - val_loss: 221.3895 - val_accuracy: 0.8000\n","Epoch 36/50\n","4/4 [==============================] - 0s 111ms/step - loss: 2.1716 - accuracy: 0.8095 - val_loss: 222.7161 - val_accuracy: 0.6000\n","Epoch 37/50\n","4/4 [==============================] - 0s 88ms/step - loss: 3.7167 - accuracy: 0.8333 - val_loss: 226.3583 - val_accuracy: 0.5333\n","Epoch 38/50\n","4/4 [==============================] - 0s 82ms/step - loss: 3.4811 - accuracy: 0.7937 - val_loss: 222.3560 - val_accuracy: 0.8000\n","Epoch 39/50\n","4/4 [==============================] - 0s 89ms/step - loss: 3.0990 - accuracy: 0.8175 - val_loss: 229.0722 - val_accuracy: 0.5333\n","Epoch 40/50\n","4/4 [==============================] - 0s 100ms/step - loss: 4.6068 - accuracy: 0.7698 - val_loss: 220.6042 - val_accuracy: 0.7333\n","Epoch 41/50\n","4/4 [==============================] - 0s 83ms/step - loss: 3.9438 - accuracy: 0.8571 - val_loss: 219.8869 - val_accuracy: 0.6667\n","Epoch 42/50\n","4/4 [==============================] - 0s 80ms/step - loss: 3.6709 - accuracy: 0.8413 - val_loss: 229.3757 - val_accuracy: 0.4667\n","Epoch 43/50\n","4/4 [==============================] - 0s 90ms/step - loss: 3.2672 - accuracy: 0.7222 - val_loss: 220.6794 - val_accuracy: 0.8000\n","Epoch 44/50\n","4/4 [==============================] - 0s 90ms/step - loss: 3.2731 - accuracy: 0.8095 - val_loss: 224.2273 - val_accuracy: 0.4667\n","Epoch 45/50\n","4/4 [==============================] - 0s 90ms/step - loss: 3.3600 - accuracy: 0.7143 - val_loss: 225.8273 - val_accuracy: 0.6000\n","Epoch 46/50\n","4/4 [==============================] - 0s 109ms/step - loss: 3.2579 - accuracy: 0.8651 - val_loss: 223.4658 - val_accuracy: 0.7333\n","Epoch 47/50\n","4/4 [==============================] - 0s 119ms/step - loss: 2.8523 - accuracy: 0.8254 - val_loss: 226.4718 - val_accuracy: 0.5333\n","Epoch 48/50\n","4/4 [==============================] - 0s 129ms/step - loss: 8.6938 - accuracy: 0.7857 - val_loss: 219.5121 - val_accuracy: 0.7333\n","Epoch 49/50\n","4/4 [==============================] - 0s 123ms/step - loss: 4.0428 - accuracy: 0.7619 - val_loss: 222.6831 - val_accuracy: 0.5333\n","Epoch 50/50\n","4/4 [==============================] - 0s 108ms/step - loss: 4.8478 - accuracy: 0.8333 - val_loss: 221.3520 - val_accuracy: 0.6000\n","2/2 [==============================] - 0s 17ms/step\n","2/2 [==============================] - 0s 23ms/step - loss: 141.7090 - accuracy: 0.9444\n","Test Loss: 141.7090, Test accuracy : 0.9444\n","Epoch 1/50\n","7/7 [==============================] - 1s 121ms/step - loss: 443.2244 - accuracy: 0.4930 - val_loss: 431.6654 - val_accuracy: 0.5000\n","Epoch 2/50\n","7/7 [==============================] - 1s 88ms/step - loss: 212.2858 - accuracy: 0.8216 - val_loss: 443.1704 - val_accuracy: 0.7083\n","Epoch 3/50\n","7/7 [==============================] - 1s 79ms/step - loss: 142.8710 - accuracy: 0.8404 - val_loss: 428.5543 - val_accuracy: 0.7500\n","Epoch 4/50\n","7/7 [==============================] - 1s 82ms/step - loss: 87.9044 - accuracy: 0.8404 - val_loss: 409.2492 - val_accuracy: 0.7500\n","Epoch 5/50\n","7/7 [==============================] - 1s 111ms/step - loss: 70.1982 - accuracy: 0.8685 - val_loss: 389.4480 - val_accuracy: 0.7500\n","Epoch 6/50\n","7/7 [==============================] - 1s 108ms/step - loss: 82.5444 - accuracy: 0.8826 - val_loss: 439.8757 - val_accuracy: 0.7500\n","Epoch 7/50\n","7/7 [==============================] - 1s 102ms/step - loss: 54.9965 - accuracy: 0.8873 - val_loss: 444.3777 - val_accuracy: 0.7500\n","Epoch 8/50\n","7/7 [==============================] - 1s 106ms/step - loss: 40.3214 - accuracy: 0.8920 - val_loss: 410.5279 - val_accuracy: 0.7500\n","Epoch 9/50\n","7/7 [==============================] - 1s 114ms/step - loss: 52.6510 - accuracy: 0.8873 - val_loss: 431.8698 - val_accuracy: 0.7500\n","Epoch 10/50\n","7/7 [==============================] - 1s 115ms/step - loss: 28.8136 - accuracy: 0.8967 - val_loss: 429.8509 - val_accuracy: 0.7500\n","Epoch 11/50\n","7/7 [==============================] - 1s 123ms/step - loss: 39.2590 - accuracy: 0.8967 - val_loss: 430.8739 - val_accuracy: 0.7500\n","Epoch 12/50\n","7/7 [==============================] - 1s 117ms/step - loss: 20.2166 - accuracy: 0.8920 - val_loss: 426.7257 - val_accuracy: 0.7500\n","Epoch 13/50\n","7/7 [==============================] - 1s 82ms/step - loss: 23.3215 - accuracy: 0.8920 - val_loss: 410.7566 - val_accuracy: 0.7500\n","Epoch 14/50\n","7/7 [==============================] - 1s 84ms/step - loss: 21.4997 - accuracy: 0.9061 - val_loss: 428.9565 - val_accuracy: 0.7500\n","Epoch 15/50\n","7/7 [==============================] - 1s 82ms/step - loss: 16.5333 - accuracy: 0.9014 - val_loss: 425.2497 - val_accuracy: 0.7500\n","Epoch 16/50\n","7/7 [==============================] - 1s 84ms/step - loss: 22.2303 - accuracy: 0.9155 - val_loss: 415.2735 - val_accuracy: 0.7500\n","Epoch 17/50\n","7/7 [==============================] - 1s 79ms/step - loss: 19.9285 - accuracy: 0.9155 - val_loss: 478.5403 - val_accuracy: 0.7500\n","Epoch 18/50\n","7/7 [==============================] - 1s 92ms/step - loss: 21.5055 - accuracy: 0.9155 - val_loss: 449.4735 - val_accuracy: 0.7500\n","Epoch 19/50\n","7/7 [==============================] - 1s 84ms/step - loss: 18.6298 - accuracy: 0.9202 - val_loss: 429.9898 - val_accuracy: 0.7500\n","Epoch 20/50\n","7/7 [==============================] - 1s 79ms/step - loss: 14.9840 - accuracy: 0.9155 - val_loss: 432.2273 - val_accuracy: 0.7500\n","Epoch 21/50\n","7/7 [==============================] - 1s 81ms/step - loss: 12.2816 - accuracy: 0.9296 - val_loss: 439.7818 - val_accuracy: 0.7500\n","Epoch 22/50\n","7/7 [==============================] - 1s 82ms/step - loss: 9.3468 - accuracy: 0.9390 - val_loss: 435.1309 - val_accuracy: 0.7500\n","Epoch 23/50\n","7/7 [==============================] - 1s 81ms/step - loss: 8.3571 - accuracy: 0.9296 - val_loss: 432.0305 - val_accuracy: 0.7500\n","Epoch 24/50\n","7/7 [==============================] - 1s 82ms/step - loss: 8.2647 - accuracy: 0.9249 - val_loss: 439.0423 - val_accuracy: 0.7500\n","Epoch 25/50\n","7/7 [==============================] - 1s 84ms/step - loss: 7.2726 - accuracy: 0.9390 - val_loss: 440.2934 - val_accuracy: 0.7500\n","Epoch 26/50\n","7/7 [==============================] - 1s 79ms/step - loss: 7.3191 - accuracy: 0.9390 - val_loss: 430.8513 - val_accuracy: 0.7500\n","Epoch 27/50\n","7/7 [==============================] - 1s 82ms/step - loss: 6.3858 - accuracy: 0.9343 - val_loss: 441.3425 - val_accuracy: 0.7500\n","Epoch 28/50\n","7/7 [==============================] - 1s 79ms/step - loss: 6.5123 - accuracy: 0.9390 - val_loss: 436.7857 - val_accuracy: 0.7500\n","Epoch 29/50\n","7/7 [==============================] - 1s 80ms/step - loss: 6.4974 - accuracy: 0.9390 - val_loss: 442.8411 - val_accuracy: 0.7500\n","Epoch 30/50\n","7/7 [==============================] - 1s 103ms/step - loss: 5.9041 - accuracy: 0.9390 - val_loss: 434.2528 - val_accuracy: 0.7500\n","Epoch 31/50\n","7/7 [==============================] - 1s 102ms/step - loss: 8.8322 - accuracy: 0.9390 - val_loss: 441.1996 - val_accuracy: 0.7500\n","Epoch 32/50\n","7/7 [==============================] - 1s 114ms/step - loss: 14.1915 - accuracy: 0.9343 - val_loss: 429.3912 - val_accuracy: 0.7500\n","Epoch 33/50\n","7/7 [==============================] - 1s 107ms/step - loss: 12.8636 - accuracy: 0.9484 - val_loss: 469.7745 - val_accuracy: 0.7500\n","Epoch 34/50\n","7/7 [==============================] - 1s 104ms/step - loss: 17.9901 - accuracy: 0.9390 - val_loss: 428.2078 - val_accuracy: 0.7500\n","Epoch 35/50\n","7/7 [==============================] - 1s 110ms/step - loss: 48.2697 - accuracy: 0.9437 - val_loss: 464.3145 - val_accuracy: 0.7500\n","Epoch 36/50\n","7/7 [==============================] - 1s 120ms/step - loss: 80.7048 - accuracy: 0.9061 - val_loss: 500.7083 - val_accuracy: 0.7083\n","Epoch 37/50\n","7/7 [==============================] - 1s 99ms/step - loss: 48.5330 - accuracy: 0.9343 - val_loss: 390.4920 - val_accuracy: 0.7500\n","Epoch 38/50\n","7/7 [==============================] - 1s 80ms/step - loss: 29.1862 - accuracy: 0.9249 - val_loss: 389.6869 - val_accuracy: 0.7500\n","Epoch 39/50\n","7/7 [==============================] - 1s 79ms/step - loss: 26.3785 - accuracy: 0.9343 - val_loss: 469.5012 - val_accuracy: 0.7500\n","Epoch 40/50\n","7/7 [==============================] - 1s 79ms/step - loss: 24.8433 - accuracy: 0.9296 - val_loss: 414.1196 - val_accuracy: 0.7500\n","Epoch 41/50\n","7/7 [==============================] - 1s 80ms/step - loss: 10.9268 - accuracy: 0.9484 - val_loss: 397.4639 - val_accuracy: 0.7500\n","Epoch 42/50\n","7/7 [==============================] - 1s 83ms/step - loss: 9.1303 - accuracy: 0.9531 - val_loss: 412.3343 - val_accuracy: 0.7083\n","Epoch 43/50\n","7/7 [==============================] - 1s 83ms/step - loss: 8.5705 - accuracy: 0.9390 - val_loss: 416.7761 - val_accuracy: 0.7500\n","Epoch 44/50\n","7/7 [==============================] - 1s 80ms/step - loss: 6.2319 - accuracy: 0.9765 - val_loss: 417.0826 - val_accuracy: 0.7083\n","Epoch 45/50\n","7/7 [==============================] - 1s 81ms/step - loss: 7.1808 - accuracy: 0.9624 - val_loss: 412.2646 - val_accuracy: 0.7083\n","Epoch 46/50\n","7/7 [==============================] - 1s 80ms/step - loss: 11.5442 - accuracy: 0.9624 - val_loss: 416.4160 - val_accuracy: 0.7083\n","Epoch 47/50\n","7/7 [==============================] - 1s 84ms/step - loss: 3.7892 - accuracy: 0.9718 - val_loss: 413.1830 - val_accuracy: 0.7083\n","Epoch 48/50\n","7/7 [==============================] - 1s 112ms/step - loss: 3.8310 - accuracy: 0.9812 - val_loss: 415.0969 - val_accuracy: 0.7083\n","Epoch 49/50\n","7/7 [==============================] - 1s 88ms/step - loss: 3.4936 - accuracy: 0.9812 - val_loss: 414.7668 - val_accuracy: 0.7500\n","Epoch 50/50\n","7/7 [==============================] - 1s 81ms/step - loss: 2.1252 - accuracy: 0.9765 - val_loss: 415.0135 - val_accuracy: 0.7083\n","2/2 [==============================] - 0s 19ms/step\n","2/2 [==============================] - 0s 19ms/step - loss: 13.5327 - accuracy: 0.9833\n","Test Loss: 13.5327, Test accuracy : 0.9833\n","Epoch 1/50\n","11/11 [==============================] - 2s 110ms/step - loss: 221.9940 - accuracy: 0.1713 - val_loss: 129.4355 - val_accuracy: 0.1944\n","Epoch 2/50\n","11/11 [==============================] - 1s 80ms/step - loss: 122.2289 - accuracy: 0.2928 - val_loss: 119.9597 - val_accuracy: 0.0833\n","Epoch 3/50\n","11/11 [==============================] - 1s 78ms/step - loss: 101.1217 - accuracy: 0.3302 - val_loss: 116.0659 - val_accuracy: 0.4444\n","Epoch 4/50\n","11/11 [==============================] - 1s 76ms/step - loss: 89.7517 - accuracy: 0.3925 - val_loss: 115.2398 - val_accuracy: 0.4722\n","Epoch 5/50\n","11/11 [==============================] - 1s 82ms/step - loss: 78.0493 - accuracy: 0.3956 - val_loss: 115.5754 - val_accuracy: 0.2778\n","Epoch 6/50\n","11/11 [==============================] - 1s 81ms/step - loss: 71.0222 - accuracy: 0.3801 - val_loss: 116.6686 - val_accuracy: 0.3056\n","Epoch 7/50\n","11/11 [==============================] - 1s 77ms/step - loss: 63.8129 - accuracy: 0.4237 - val_loss: 117.0467 - val_accuracy: 0.3333\n","Epoch 8/50\n","11/11 [==============================] - 1s 77ms/step - loss: 56.7639 - accuracy: 0.4361 - val_loss: 108.6469 - val_accuracy: 0.4444\n","Epoch 9/50\n","11/11 [==============================] - 1s 78ms/step - loss: 53.6525 - accuracy: 0.4673 - val_loss: 99.0362 - val_accuracy: 0.3333\n","Epoch 10/50\n","11/11 [==============================] - 1s 98ms/step - loss: 48.5395 - accuracy: 0.4393 - val_loss: 85.4116 - val_accuracy: 0.3333\n","Epoch 11/50\n","11/11 [==============================] - 1s 101ms/step - loss: 44.4960 - accuracy: 0.4984 - val_loss: 79.0933 - val_accuracy: 0.3333\n","Epoch 12/50\n","11/11 [==============================] - 1s 106ms/step - loss: 34.8717 - accuracy: 0.4829 - val_loss: 76.8982 - val_accuracy: 0.3611\n","Epoch 13/50\n","11/11 [==============================] - 1s 108ms/step - loss: 33.9078 - accuracy: 0.4953 - val_loss: 78.0956 - val_accuracy: 0.2500\n","Epoch 14/50\n","11/11 [==============================] - 1s 103ms/step - loss: 31.7944 - accuracy: 0.4673 - val_loss: 76.6554 - val_accuracy: 0.3333\n","Epoch 15/50\n","11/11 [==============================] - 1s 90ms/step - loss: 29.2051 - accuracy: 0.5047 - val_loss: 72.8667 - val_accuracy: 0.3056\n","Epoch 16/50\n","11/11 [==============================] - 1s 75ms/step - loss: 27.0274 - accuracy: 0.5234 - val_loss: 81.0019 - val_accuracy: 0.2500\n","Epoch 17/50\n","11/11 [==============================] - 1s 79ms/step - loss: 27.0826 - accuracy: 0.5483 - val_loss: 79.7021 - val_accuracy: 0.2778\n","Epoch 18/50\n","11/11 [==============================] - 1s 80ms/step - loss: 23.4463 - accuracy: 0.5140 - val_loss: 70.5959 - val_accuracy: 0.2500\n","Epoch 19/50\n","11/11 [==============================] - 1s 77ms/step - loss: 21.0514 - accuracy: 0.5140 - val_loss: 71.7308 - val_accuracy: 0.3333\n","Epoch 20/50\n","11/11 [==============================] - 1s 75ms/step - loss: 19.4659 - accuracy: 0.5202 - val_loss: 68.3548 - val_accuracy: 0.4444\n","Epoch 21/50\n","11/11 [==============================] - 1s 78ms/step - loss: 18.7909 - accuracy: 0.5545 - val_loss: 68.8701 - val_accuracy: 0.3889\n","Epoch 22/50\n","11/11 [==============================] - 1s 75ms/step - loss: 17.7023 - accuracy: 0.5140 - val_loss: 74.0365 - val_accuracy: 0.3056\n","Epoch 23/50\n","11/11 [==============================] - 1s 78ms/step - loss: 17.0501 - accuracy: 0.5639 - val_loss: 72.2909 - val_accuracy: 0.3889\n","Epoch 24/50\n","11/11 [==============================] - 1s 76ms/step - loss: 16.6169 - accuracy: 0.5421 - val_loss: 72.4769 - val_accuracy: 0.4444\n","Epoch 25/50\n","11/11 [==============================] - 1s 79ms/step - loss: 17.4274 - accuracy: 0.5639 - val_loss: 68.7974 - val_accuracy: 0.3056\n","Epoch 26/50\n","11/11 [==============================] - 1s 79ms/step - loss: 16.6247 - accuracy: 0.5140 - val_loss: 72.4569 - val_accuracy: 0.4444\n","Epoch 27/50\n","11/11 [==============================] - 1s 96ms/step - loss: 18.7467 - accuracy: 0.5421 - val_loss: 82.6884 - val_accuracy: 0.5278\n","Epoch 28/50\n","11/11 [==============================] - 1s 100ms/step - loss: 19.6184 - accuracy: 0.5109 - val_loss: 74.8279 - val_accuracy: 0.4444\n","Epoch 29/50\n","11/11 [==============================] - 1s 106ms/step - loss: 18.8066 - accuracy: 0.4922 - val_loss: 74.2304 - val_accuracy: 0.3611\n","Epoch 30/50\n","11/11 [==============================] - 1s 106ms/step - loss: 16.2869 - accuracy: 0.5109 - val_loss: 72.4458 - val_accuracy: 0.4167\n","Epoch 31/50\n","11/11 [==============================] - 1s 110ms/step - loss: 15.8220 - accuracy: 0.4891 - val_loss: 69.1502 - val_accuracy: 0.3889\n","Epoch 32/50\n","11/11 [==============================] - 1s 91ms/step - loss: 15.5135 - accuracy: 0.5327 - val_loss: 73.1626 - val_accuracy: 0.4444\n","Epoch 33/50\n","11/11 [==============================] - 1s 78ms/step - loss: 14.1019 - accuracy: 0.4922 - val_loss: 71.3662 - val_accuracy: 0.4167\n","Epoch 34/50\n","11/11 [==============================] - 1s 79ms/step - loss: 13.6753 - accuracy: 0.5234 - val_loss: 72.3584 - val_accuracy: 0.3611\n","Epoch 35/50\n","11/11 [==============================] - 1s 75ms/step - loss: 13.2621 - accuracy: 0.4953 - val_loss: 73.5172 - val_accuracy: 0.3056\n","Epoch 36/50\n","11/11 [==============================] - 1s 76ms/step - loss: 11.9467 - accuracy: 0.5109 - val_loss: 69.6502 - val_accuracy: 0.4167\n","Epoch 37/50\n","11/11 [==============================] - 1s 83ms/step - loss: 11.4235 - accuracy: 0.5607 - val_loss: 72.0489 - val_accuracy: 0.4167\n","Epoch 38/50\n","11/11 [==============================] - 1s 75ms/step - loss: 11.8161 - accuracy: 0.5670 - val_loss: 76.1246 - val_accuracy: 0.4167\n","Epoch 39/50\n","11/11 [==============================] - 1s 75ms/step - loss: 12.9754 - accuracy: 0.5202 - val_loss: 70.4862 - val_accuracy: 0.3889\n","Epoch 40/50\n","11/11 [==============================] - 1s 79ms/step - loss: 12.7712 - accuracy: 0.5483 - val_loss: 71.8214 - val_accuracy: 0.4722\n","Epoch 41/50\n","11/11 [==============================] - 1s 75ms/step - loss: 15.5027 - accuracy: 0.4922 - val_loss: 68.8340 - val_accuracy: 0.4167\n","Epoch 42/50\n","11/11 [==============================] - 1s 82ms/step - loss: 14.8817 - accuracy: 0.4704 - val_loss: 67.9656 - val_accuracy: 0.3889\n","Epoch 43/50\n","11/11 [==============================] - 1s 78ms/step - loss: 23.9856 - accuracy: 0.4766 - val_loss: 72.5634 - val_accuracy: 0.4444\n","Epoch 44/50\n","11/11 [==============================] - 1s 104ms/step - loss: 30.2471 - accuracy: 0.4486 - val_loss: 70.4300 - val_accuracy: 0.4167\n","Epoch 45/50\n","11/11 [==============================] - 1s 103ms/step - loss: 28.5501 - accuracy: 0.4112 - val_loss: 72.1326 - val_accuracy: 0.4722\n","Epoch 46/50\n","11/11 [==============================] - 1s 103ms/step - loss: 22.0440 - accuracy: 0.4579 - val_loss: 73.0110 - val_accuracy: 0.5000\n","Epoch 47/50\n","11/11 [==============================] - 1s 110ms/step - loss: 19.3495 - accuracy: 0.4860 - val_loss: 75.1446 - val_accuracy: 0.3333\n","Epoch 48/50\n","11/11 [==============================] - 1s 107ms/step - loss: 18.2979 - accuracy: 0.4081 - val_loss: 67.8927 - val_accuracy: 0.3889\n","Epoch 49/50\n","11/11 [==============================] - 1s 76ms/step - loss: 17.8642 - accuracy: 0.4486 - val_loss: 85.6491 - val_accuracy: 0.4722\n","Epoch 50/50\n","11/11 [==============================] - 1s 78ms/step - loss: 24.0389 - accuracy: 0.4050 - val_loss: 81.2774 - val_accuracy: 0.3333\n","3/3 [==============================] - 0s 24ms/step\n","3/3 [==============================] - 0s 23ms/step - loss: 0.8618 - accuracy: 0.9111\n","Test Loss: 0.8618, Test accuracy : 0.9111\n","Epoch 1/50\n","10/10 [==============================] - 2s 109ms/step - loss: 232.2369 - accuracy: 0.3453 - val_loss: 307.8728 - val_accuracy: 0.5429\n","Epoch 2/50\n","10/10 [==============================] - 1s 82ms/step - loss: 147.3453 - accuracy: 0.3974 - val_loss: 295.5700 - val_accuracy: 0.3429\n","Epoch 3/50\n","10/10 [==============================] - 1s 79ms/step - loss: 119.8646 - accuracy: 0.3388 - val_loss: 313.6004 - val_accuracy: 0.4000\n","Epoch 4/50\n","10/10 [==============================] - 1s 84ms/step - loss: 98.6016 - accuracy: 0.4691 - val_loss: 306.0318 - val_accuracy: 0.4286\n","Epoch 5/50\n","10/10 [==============================] - 1s 84ms/step - loss: 86.5013 - accuracy: 0.5081 - val_loss: 291.8931 - val_accuracy: 0.2857\n","Epoch 6/50\n","10/10 [==============================] - 1s 106ms/step - loss: 73.3882 - accuracy: 0.4951 - val_loss: 296.0555 - val_accuracy: 0.5429\n","Epoch 7/50\n","10/10 [==============================] - 1s 105ms/step - loss: 69.6852 - accuracy: 0.5798 - val_loss: 280.8384 - val_accuracy: 0.4571\n","Epoch 8/50\n","10/10 [==============================] - 1s 137ms/step - loss: 71.7322 - accuracy: 0.5309 - val_loss: 298.2866 - val_accuracy: 0.4571\n","Epoch 9/50\n","10/10 [==============================] - 1s 138ms/step - loss: 64.6324 - accuracy: 0.5472 - val_loss: 275.7623 - val_accuracy: 0.4000\n","Epoch 10/50\n","10/10 [==============================] - 1s 131ms/step - loss: 93.6451 - accuracy: 0.5505 - val_loss: 292.6199 - val_accuracy: 0.5143\n","Epoch 11/50\n","10/10 [==============================] - 1s 111ms/step - loss: 59.5803 - accuracy: 0.5831 - val_loss: 291.8535 - val_accuracy: 0.5429\n","Epoch 12/50\n","10/10 [==============================] - 1s 126ms/step - loss: 55.3280 - accuracy: 0.6221 - val_loss: 297.2944 - val_accuracy: 0.6000\n","Epoch 13/50\n","10/10 [==============================] - 1s 111ms/step - loss: 52.3398 - accuracy: 0.6319 - val_loss: 290.8502 - val_accuracy: 0.4286\n","Epoch 14/50\n","10/10 [==============================] - 1s 97ms/step - loss: 49.8970 - accuracy: 0.5961 - val_loss: 296.3459 - val_accuracy: 0.5714\n","Epoch 15/50\n","10/10 [==============================] - 1s 79ms/step - loss: 50.6896 - accuracy: 0.6547 - val_loss: 290.4620 - val_accuracy: 0.5143\n","Epoch 16/50\n","10/10 [==============================] - 1s 78ms/step - loss: 49.0869 - accuracy: 0.6221 - val_loss: 302.0785 - val_accuracy: 0.6000\n","Epoch 17/50\n","10/10 [==============================] - 1s 81ms/step - loss: 45.4596 - accuracy: 0.6287 - val_loss: 288.8801 - val_accuracy: 0.5429\n","Epoch 18/50\n","10/10 [==============================] - 1s 79ms/step - loss: 42.5746 - accuracy: 0.6580 - val_loss: 302.6081 - val_accuracy: 0.5143\n","Epoch 19/50\n","10/10 [==============================] - 1s 78ms/step - loss: 44.1980 - accuracy: 0.6612 - val_loss: 289.1944 - val_accuracy: 0.5143\n","Epoch 20/50\n","10/10 [==============================] - 1s 82ms/step - loss: 42.8895 - accuracy: 0.6873 - val_loss: 301.7518 - val_accuracy: 0.6286\n","Epoch 21/50\n","10/10 [==============================] - 1s 76ms/step - loss: 39.9756 - accuracy: 0.6808 - val_loss: 302.6667 - val_accuracy: 0.5429\n","Epoch 22/50\n","10/10 [==============================] - 1s 84ms/step - loss: 36.9749 - accuracy: 0.7394 - val_loss: 302.8878 - val_accuracy: 0.5429\n","Epoch 23/50\n","10/10 [==============================] - 1s 80ms/step - loss: 31.6657 - accuracy: 0.6906 - val_loss: 300.0688 - val_accuracy: 0.5429\n","Epoch 24/50\n","10/10 [==============================] - 1s 81ms/step - loss: 29.1441 - accuracy: 0.7068 - val_loss: 306.1516 - val_accuracy: 0.5143\n","Epoch 25/50\n","10/10 [==============================] - 1s 80ms/step - loss: 27.6621 - accuracy: 0.7134 - val_loss: 300.5543 - val_accuracy: 0.5429\n","Epoch 26/50\n","10/10 [==============================] - 1s 79ms/step - loss: 27.3953 - accuracy: 0.7166 - val_loss: 304.3706 - val_accuracy: 0.6286\n","Epoch 27/50\n","10/10 [==============================] - 1s 104ms/step - loss: 26.5842 - accuracy: 0.7101 - val_loss: 305.0852 - val_accuracy: 0.5429\n","Epoch 28/50\n","10/10 [==============================] - 1s 113ms/step - loss: 27.0136 - accuracy: 0.7199 - val_loss: 319.2137 - val_accuracy: 0.6000\n","Epoch 29/50\n","10/10 [==============================] - 1s 110ms/step - loss: 29.6933 - accuracy: 0.7101 - val_loss: 313.9185 - val_accuracy: 0.6000\n","Epoch 30/50\n","10/10 [==============================] - 1s 106ms/step - loss: 32.2664 - accuracy: 0.7199 - val_loss: 321.3116 - val_accuracy: 0.6000\n","Epoch 31/50\n","10/10 [==============================] - 1s 105ms/step - loss: 32.6793 - accuracy: 0.7557 - val_loss: 317.2927 - val_accuracy: 0.4571\n","Epoch 32/50\n","10/10 [==============================] - 1s 110ms/step - loss: 26.1665 - accuracy: 0.7394 - val_loss: 314.0298 - val_accuracy: 0.6000\n","Epoch 33/50\n","10/10 [==============================] - 1s 81ms/step - loss: 22.7407 - accuracy: 0.7394 - val_loss: 340.7345 - val_accuracy: 0.4857\n","Epoch 34/50\n","10/10 [==============================] - 1s 83ms/step - loss: 25.5415 - accuracy: 0.7329 - val_loss: 325.3822 - val_accuracy: 0.5143\n","Epoch 35/50\n","10/10 [==============================] - 1s 79ms/step - loss: 20.7625 - accuracy: 0.7362 - val_loss: 321.7149 - val_accuracy: 0.5714\n","Epoch 36/50\n","10/10 [==============================] - 1s 80ms/step - loss: 28.5431 - accuracy: 0.7622 - val_loss: 319.5193 - val_accuracy: 0.5143\n","Epoch 37/50\n","10/10 [==============================] - 1s 88ms/step - loss: 31.2696 - accuracy: 0.7524 - val_loss: 331.7501 - val_accuracy: 0.6000\n","Epoch 38/50\n","10/10 [==============================] - 1s 82ms/step - loss: 27.2020 - accuracy: 0.7492 - val_loss: 310.5672 - val_accuracy: 0.5143\n","Epoch 39/50\n","10/10 [==============================] - 1s 82ms/step - loss: 29.2659 - accuracy: 0.7720 - val_loss: 329.6363 - val_accuracy: 0.5429\n","Epoch 40/50\n","10/10 [==============================] - 1s 80ms/step - loss: 23.8628 - accuracy: 0.7459 - val_loss: 307.3163 - val_accuracy: 0.6286\n","Epoch 41/50\n","10/10 [==============================] - 1s 89ms/step - loss: 21.6611 - accuracy: 0.7590 - val_loss: 326.2139 - val_accuracy: 0.5429\n","Epoch 42/50\n","10/10 [==============================] - 1s 81ms/step - loss: 21.7828 - accuracy: 0.7590 - val_loss: 311.8922 - val_accuracy: 0.5143\n","Epoch 43/50\n","10/10 [==============================] - 1s 82ms/step - loss: 19.3479 - accuracy: 0.7818 - val_loss: 323.7581 - val_accuracy: 0.5429\n","Epoch 44/50\n","10/10 [==============================] - 1s 84ms/step - loss: 18.9520 - accuracy: 0.7785 - val_loss: 317.2160 - val_accuracy: 0.6000\n","Epoch 45/50\n","10/10 [==============================] - 1s 116ms/step - loss: 16.1117 - accuracy: 0.7818 - val_loss: 323.2606 - val_accuracy: 0.5143\n","Epoch 46/50\n","10/10 [==============================] - 1s 114ms/step - loss: 16.5826 - accuracy: 0.7883 - val_loss: 318.5798 - val_accuracy: 0.5714\n","Epoch 47/50\n","10/10 [==============================] - 1s 110ms/step - loss: 14.1005 - accuracy: 0.7557 - val_loss: 318.5023 - val_accuracy: 0.5429\n","Epoch 48/50\n","10/10 [==============================] - 1s 104ms/step - loss: 13.5945 - accuracy: 0.7720 - val_loss: 316.0748 - val_accuracy: 0.5429\n","Epoch 49/50\n","10/10 [==============================] - 1s 110ms/step - loss: 13.6245 - accuracy: 0.7883 - val_loss: 317.2565 - val_accuracy: 0.5143\n","Epoch 50/50\n","10/10 [==============================] - 1s 91ms/step - loss: 12.7840 - accuracy: 0.7752 - val_loss: 317.0009 - val_accuracy: 0.6000\n","3/3 [==============================] - 0s 23ms/step\n","3/3 [==============================] - 0s 31ms/step - loss: 2.7624 - accuracy: 0.9770\n","Test Loss: 2.7624, Test accuracy : 0.9770\n","Epoch 1/50\n","16/16 [==============================] - 3s 131ms/step - loss: 164.2443 - accuracy: 0.2484 - val_loss: 294.8943 - val_accuracy: 0.2222\n","Epoch 2/50\n","16/16 [==============================] - 2s 96ms/step - loss: 106.6325 - accuracy: 0.2981 - val_loss: 276.8474 - val_accuracy: 0.3148\n","Epoch 3/50\n","16/16 [==============================] - 1s 83ms/step - loss: 92.6653 - accuracy: 0.2712 - val_loss: 284.1307 - val_accuracy: 0.1111\n","Epoch 4/50\n","16/16 [==============================] - 1s 87ms/step - loss: 78.3368 - accuracy: 0.3602 - val_loss: 281.3268 - val_accuracy: 0.3333\n","Epoch 5/50\n","16/16 [==============================] - 1s 81ms/step - loss: 67.6162 - accuracy: 0.3313 - val_loss: 263.7313 - val_accuracy: 0.1852\n","Epoch 6/50\n","16/16 [==============================] - 1s 84ms/step - loss: 61.0484 - accuracy: 0.3644 - val_loss: 266.4926 - val_accuracy: 0.3333\n","Epoch 7/50\n","16/16 [==============================] - 1s 84ms/step - loss: 52.3097 - accuracy: 0.3043 - val_loss: 265.6788 - val_accuracy: 0.4074\n","Epoch 8/50\n","16/16 [==============================] - 1s 80ms/step - loss: 48.6369 - accuracy: 0.3499 - val_loss: 258.9301 - val_accuracy: 0.2407\n","Epoch 9/50\n","16/16 [==============================] - 1s 91ms/step - loss: 47.2947 - accuracy: 0.3375 - val_loss: 265.3857 - val_accuracy: 0.2778\n","Epoch 10/50\n","16/16 [==============================] - 2s 114ms/step - loss: 42.2052 - accuracy: 0.3251 - val_loss: 262.9220 - val_accuracy: 0.3148\n","Epoch 11/50\n","16/16 [==============================] - 2s 107ms/step - loss: 35.8277 - accuracy: 0.3892 - val_loss: 261.9815 - val_accuracy: 0.2037\n","Epoch 12/50\n","16/16 [==============================] - 2s 118ms/step - loss: 31.3962 - accuracy: 0.3292 - val_loss: 260.7499 - val_accuracy: 0.1667\n","Epoch 13/50\n","16/16 [==============================] - 1s 84ms/step - loss: 29.3003 - accuracy: 0.3561 - val_loss: 262.0652 - val_accuracy: 0.2037\n","Epoch 14/50\n","16/16 [==============================] - 1s 79ms/step - loss: 29.9439 - accuracy: 0.3354 - val_loss: 262.9573 - val_accuracy: 0.1296\n","Epoch 15/50\n","16/16 [==============================] - 1s 80ms/step - loss: 31.6237 - accuracy: 0.3520 - val_loss: 252.1138 - val_accuracy: 0.1481\n","Epoch 16/50\n","16/16 [==============================] - 1s 81ms/step - loss: 27.7397 - accuracy: 0.3706 - val_loss: 260.9533 - val_accuracy: 0.1296\n","Epoch 17/50\n","16/16 [==============================] - 1s 79ms/step - loss: 36.7619 - accuracy: 0.3623 - val_loss: 263.3617 - val_accuracy: 0.0926\n","Epoch 18/50\n","16/16 [==============================] - 1s 81ms/step - loss: 46.2638 - accuracy: 0.3230 - val_loss: 262.2259 - val_accuracy: 0.1667\n","Epoch 19/50\n","16/16 [==============================] - 1s 80ms/step - loss: 37.9509 - accuracy: 0.3333 - val_loss: 272.8789 - val_accuracy: 0.1852\n","Epoch 20/50\n","16/16 [==============================] - 1s 85ms/step - loss: 24.4943 - accuracy: 0.3561 - val_loss: 262.2081 - val_accuracy: 0.1296\n","Epoch 21/50\n","16/16 [==============================] - 2s 114ms/step - loss: 23.0085 - accuracy: 0.3333 - val_loss: 267.3128 - val_accuracy: 0.1852\n","Epoch 22/50\n","16/16 [==============================] - 2s 101ms/step - loss: 21.7418 - accuracy: 0.3830 - val_loss: 263.2938 - val_accuracy: 0.2222\n","Epoch 23/50\n","16/16 [==============================] - 2s 114ms/step - loss: 22.1797 - accuracy: 0.3851 - val_loss: 267.9485 - val_accuracy: 0.1852\n","Epoch 24/50\n","16/16 [==============================] - 1s 90ms/step - loss: 23.4189 - accuracy: 0.3582 - val_loss: 257.9100 - val_accuracy: 0.3704\n","Epoch 25/50\n","16/16 [==============================] - 1s 83ms/step - loss: 45.9654 - accuracy: 0.3644 - val_loss: 286.1961 - val_accuracy: 0.3148\n","Epoch 26/50\n","16/16 [==============================] - 1s 79ms/step - loss: 24.0130 - accuracy: 0.4017 - val_loss: 252.4659 - val_accuracy: 0.2593\n","Epoch 27/50\n","16/16 [==============================] - 1s 83ms/step - loss: 19.0477 - accuracy: 0.4161 - val_loss: 268.7759 - val_accuracy: 0.2222\n","Epoch 28/50\n","16/16 [==============================] - 1s 81ms/step - loss: 17.1195 - accuracy: 0.4099 - val_loss: 259.7462 - val_accuracy: 0.3519\n","Epoch 29/50\n","16/16 [==============================] - 1s 85ms/step - loss: 16.6615 - accuracy: 0.4410 - val_loss: 268.1603 - val_accuracy: 0.2037\n","Epoch 30/50\n","16/16 [==============================] - 1s 87ms/step - loss: 17.4275 - accuracy: 0.3830 - val_loss: 269.2047 - val_accuracy: 0.2593\n","Epoch 31/50\n","16/16 [==============================] - 1s 83ms/step - loss: 15.4990 - accuracy: 0.3727 - val_loss: 266.7152 - val_accuracy: 0.3148\n","Epoch 32/50\n","16/16 [==============================] - 2s 110ms/step - loss: 13.7224 - accuracy: 0.3768 - val_loss: 269.5374 - val_accuracy: 0.2593\n","Epoch 33/50\n","16/16 [==============================] - 2s 108ms/step - loss: 13.1056 - accuracy: 0.3913 - val_loss: 265.0160 - val_accuracy: 0.2963\n","Epoch 34/50\n","16/16 [==============================] - 2s 123ms/step - loss: 12.8587 - accuracy: 0.3727 - val_loss: 268.7508 - val_accuracy: 0.2407\n","Epoch 35/50\n","16/16 [==============================] - 2s 97ms/step - loss: 12.3117 - accuracy: 0.3830 - val_loss: 268.5516 - val_accuracy: 0.2963\n","Epoch 36/50\n","16/16 [==============================] - 1s 84ms/step - loss: 11.9107 - accuracy: 0.3727 - val_loss: 267.2092 - val_accuracy: 0.2407\n","Epoch 37/50\n","16/16 [==============================] - 1s 83ms/step - loss: 12.4397 - accuracy: 0.4079 - val_loss: 264.7603 - val_accuracy: 0.2407\n","Epoch 38/50\n","16/16 [==============================] - 1s 83ms/step - loss: 12.3551 - accuracy: 0.3830 - val_loss: 268.5701 - val_accuracy: 0.3333\n","Epoch 39/50\n","16/16 [==============================] - 1s 86ms/step - loss: 11.9204 - accuracy: 0.3954 - val_loss: 265.9920 - val_accuracy: 0.3148\n","Epoch 40/50\n","16/16 [==============================] - 1s 87ms/step - loss: 11.9839 - accuracy: 0.3623 - val_loss: 270.9096 - val_accuracy: 0.1852\n","Epoch 41/50\n","16/16 [==============================] - 1s 84ms/step - loss: 11.7367 - accuracy: 0.3209 - val_loss: 265.4651 - val_accuracy: 0.2407\n","Epoch 42/50\n","16/16 [==============================] - 1s 86ms/step - loss: 11.5166 - accuracy: 0.3085 - val_loss: 269.6656 - val_accuracy: 0.3148\n","Epoch 43/50\n","16/16 [==============================] - 2s 116ms/step - loss: 10.7451 - accuracy: 0.3188 - val_loss: 265.3344 - val_accuracy: 0.3333\n","Epoch 44/50\n","16/16 [==============================] - 2s 113ms/step - loss: 10.6687 - accuracy: 0.3747 - val_loss: 266.9395 - val_accuracy: 0.2222\n","Epoch 45/50\n","16/16 [==============================] - 2s 110ms/step - loss: 11.5152 - accuracy: 0.3147 - val_loss: 267.0944 - val_accuracy: 0.2593\n","Epoch 46/50\n","16/16 [==============================] - 2s 94ms/step - loss: 13.0625 - accuracy: 0.3934 - val_loss: 267.0313 - val_accuracy: 0.2407\n","Epoch 47/50\n","16/16 [==============================] - 1s 81ms/step - loss: 11.5479 - accuracy: 0.3271 - val_loss: 269.7303 - val_accuracy: 0.2778\n","Epoch 48/50\n","16/16 [==============================] - 1s 80ms/step - loss: 12.1247 - accuracy: 0.3478 - val_loss: 266.5274 - val_accuracy: 0.1481\n","Epoch 49/50\n","16/16 [==============================] - 1s 83ms/step - loss: 18.6967 - accuracy: 0.2816 - val_loss: 268.7909 - val_accuracy: 0.2407\n","Epoch 50/50\n","16/16 [==============================] - 1s 80ms/step - loss: 17.3934 - accuracy: 0.2402 - val_loss: 267.3011 - val_accuracy: 0.2222\n","5/5 [==============================] - 0s 17ms/step\n","5/5 [==============================] - 0s 16ms/step - loss: 208.6373 - accuracy: 0.9407\n","Test Loss: 208.6373, Test accuracy : 0.9407\n","Epoch 1/50\n","7/7 [==============================] - 2s 153ms/step - loss: 281.2884 - accuracy: 0.2634 - val_loss: 199.5286 - val_accuracy: 0.0435\n","Epoch 2/50\n","7/7 [==============================] - 1s 125ms/step - loss: 138.7899 - accuracy: 0.3122 - val_loss: 156.2451 - val_accuracy: 0.2174\n","Epoch 3/50\n","7/7 [==============================] - 1s 112ms/step - loss: 123.1826 - accuracy: 0.4341 - val_loss: 161.2489 - val_accuracy: 0.2174\n","Epoch 4/50\n","7/7 [==============================] - 1s 115ms/step - loss: 102.5389 - accuracy: 0.4732 - val_loss: 156.4335 - val_accuracy: 0.2174\n","Epoch 5/50\n","7/7 [==============================] - 1s 108ms/step - loss: 88.8510 - accuracy: 0.4244 - val_loss: 156.6769 - val_accuracy: 0.2174\n","Epoch 6/50\n","7/7 [==============================] - 1s 115ms/step - loss: 77.2677 - accuracy: 0.4829 - val_loss: 151.3575 - val_accuracy: 0.2174\n","Epoch 7/50\n","7/7 [==============================] - 1s 94ms/step - loss: 69.6198 - accuracy: 0.4732 - val_loss: 149.2061 - val_accuracy: 0.2174\n","Epoch 8/50\n","7/7 [==============================] - 1s 84ms/step - loss: 62.9620 - accuracy: 0.5171 - val_loss: 151.1458 - val_accuracy: 0.2174\n","Epoch 9/50\n","7/7 [==============================] - 1s 83ms/step - loss: 57.2963 - accuracy: 0.5268 - val_loss: 145.9486 - val_accuracy: 0.2174\n","Epoch 10/50\n","7/7 [==============================] - 1s 84ms/step - loss: 53.0745 - accuracy: 0.5805 - val_loss: 145.3395 - val_accuracy: 0.2174\n","Epoch 11/50\n","7/7 [==============================] - 1s 82ms/step - loss: 48.6380 - accuracy: 0.5122 - val_loss: 146.0863 - val_accuracy: 0.2174\n","Epoch 12/50\n","7/7 [==============================] - 1s 78ms/step - loss: 45.3990 - accuracy: 0.5220 - val_loss: 140.8005 - val_accuracy: 0.2174\n","Epoch 13/50\n","7/7 [==============================] - 1s 84ms/step - loss: 43.7690 - accuracy: 0.6293 - val_loss: 142.5828 - val_accuracy: 0.2174\n","Epoch 14/50\n","7/7 [==============================] - 1s 87ms/step - loss: 40.8087 - accuracy: 0.5610 - val_loss: 139.8099 - val_accuracy: 0.2174\n","Epoch 15/50\n","7/7 [==============================] - 1s 90ms/step - loss: 38.5535 - accuracy: 0.5951 - val_loss: 136.1694 - val_accuracy: 0.2174\n","Epoch 16/50\n","7/7 [==============================] - 1s 85ms/step - loss: 36.2640 - accuracy: 0.6244 - val_loss: 135.9803 - val_accuracy: 0.2174\n","Epoch 17/50\n","7/7 [==============================] - 1s 89ms/step - loss: 34.5630 - accuracy: 0.6293 - val_loss: 137.4593 - val_accuracy: 0.2174\n","Epoch 18/50\n","7/7 [==============================] - 1s 90ms/step - loss: 33.4412 - accuracy: 0.6195 - val_loss: 134.4935 - val_accuracy: 0.2174\n","Epoch 19/50\n","7/7 [==============================] - 1s 80ms/step - loss: 31.9840 - accuracy: 0.6439 - val_loss: 136.0686 - val_accuracy: 0.2174\n","Epoch 20/50\n","7/7 [==============================] - 1s 90ms/step - loss: 30.8027 - accuracy: 0.6244 - val_loss: 133.7891 - val_accuracy: 0.2174\n","Epoch 21/50\n","7/7 [==============================] - 1s 92ms/step - loss: 29.6232 - accuracy: 0.6683 - val_loss: 133.6512 - val_accuracy: 0.2174\n","Epoch 22/50\n","7/7 [==============================] - 1s 82ms/step - loss: 28.5293 - accuracy: 0.6390 - val_loss: 134.4212 - val_accuracy: 0.2174\n","Epoch 23/50\n","7/7 [==============================] - 1s 113ms/step - loss: 27.4168 - accuracy: 0.6829 - val_loss: 133.0674 - val_accuracy: 0.2174\n","Epoch 24/50\n","7/7 [==============================] - 1s 112ms/step - loss: 26.5826 - accuracy: 0.6488 - val_loss: 133.2531 - val_accuracy: 0.2174\n","Epoch 25/50\n","7/7 [==============================] - 1s 105ms/step - loss: 25.8135 - accuracy: 0.6927 - val_loss: 132.0187 - val_accuracy: 0.2174\n","Epoch 26/50\n","7/7 [==============================] - 1s 109ms/step - loss: 25.1440 - accuracy: 0.6780 - val_loss: 132.2562 - val_accuracy: 0.2174\n","Epoch 27/50\n","7/7 [==============================] - 1s 105ms/step - loss: 24.5286 - accuracy: 0.7171 - val_loss: 131.1694 - val_accuracy: 0.2174\n","Epoch 28/50\n","7/7 [==============================] - 1s 105ms/step - loss: 24.4577 - accuracy: 0.6732 - val_loss: 130.7054 - val_accuracy: 0.2174\n","Epoch 29/50\n","7/7 [==============================] - 1s 119ms/step - loss: 23.8652 - accuracy: 0.6976 - val_loss: 129.6037 - val_accuracy: 0.2174\n","Epoch 30/50\n","7/7 [==============================] - 1s 106ms/step - loss: 23.3697 - accuracy: 0.7171 - val_loss: 132.0467 - val_accuracy: 0.2174\n","Epoch 31/50\n","7/7 [==============================] - 1s 111ms/step - loss: 22.1048 - accuracy: 0.7220 - val_loss: 131.6839 - val_accuracy: 0.2174\n","Epoch 32/50\n","7/7 [==============================] - 1s 83ms/step - loss: 21.4842 - accuracy: 0.7073 - val_loss: 132.4648 - val_accuracy: 0.2174\n","Epoch 33/50\n","7/7 [==============================] - 1s 86ms/step - loss: 20.1779 - accuracy: 0.6976 - val_loss: 131.2935 - val_accuracy: 0.2174\n","Epoch 34/50\n","7/7 [==============================] - 1s 87ms/step - loss: 19.7791 - accuracy: 0.6927 - val_loss: 127.9061 - val_accuracy: 0.2174\n","Epoch 35/50\n","7/7 [==============================] - 1s 86ms/step - loss: 19.6384 - accuracy: 0.7073 - val_loss: 133.1816 - val_accuracy: 0.2174\n","Epoch 36/50\n","7/7 [==============================] - 1s 80ms/step - loss: 18.9729 - accuracy: 0.6927 - val_loss: 129.0173 - val_accuracy: 0.2174\n","Epoch 37/50\n","7/7 [==============================] - 1s 86ms/step - loss: 18.8405 - accuracy: 0.7317 - val_loss: 132.2844 - val_accuracy: 0.2174\n","Epoch 38/50\n","7/7 [==============================] - 1s 81ms/step - loss: 17.2291 - accuracy: 0.7171 - val_loss: 130.5723 - val_accuracy: 0.2174\n","Epoch 39/50\n","7/7 [==============================] - 1s 81ms/step - loss: 16.4604 - accuracy: 0.7024 - val_loss: 132.9183 - val_accuracy: 0.2174\n","Epoch 40/50\n","7/7 [==============================] - 1s 83ms/step - loss: 16.1022 - accuracy: 0.6976 - val_loss: 130.6055 - val_accuracy: 0.2174\n","Epoch 41/50\n","7/7 [==============================] - 1s 82ms/step - loss: 15.5801 - accuracy: 0.7220 - val_loss: 132.3202 - val_accuracy: 0.2174\n","Epoch 42/50\n","7/7 [==============================] - 1s 82ms/step - loss: 15.2359 - accuracy: 0.7171 - val_loss: 132.3231 - val_accuracy: 0.2174\n","Epoch 43/50\n","7/7 [==============================] - 1s 79ms/step - loss: 14.9851 - accuracy: 0.7317 - val_loss: 132.1279 - val_accuracy: 0.2174\n","Epoch 44/50\n","7/7 [==============================] - 1s 83ms/step - loss: 15.0476 - accuracy: 0.6976 - val_loss: 131.3135 - val_accuracy: 0.2174\n","Epoch 45/50\n","7/7 [==============================] - 1s 78ms/step - loss: 14.6079 - accuracy: 0.6341 - val_loss: 132.1199 - val_accuracy: 0.2174\n","Epoch 46/50\n","7/7 [==============================] - 1s 82ms/step - loss: 14.3874 - accuracy: 0.7415 - val_loss: 131.6495 - val_accuracy: 0.2174\n","Epoch 47/50\n","7/7 [==============================] - 1s 79ms/step - loss: 14.4750 - accuracy: 0.6732 - val_loss: 132.7743 - val_accuracy: 0.2174\n","Epoch 48/50\n","7/7 [==============================] - 1s 82ms/step - loss: 14.0752 - accuracy: 0.6341 - val_loss: 132.3730 - val_accuracy: 0.2174\n","Epoch 49/50\n","7/7 [==============================] - 1s 102ms/step - loss: 13.6687 - accuracy: 0.6683 - val_loss: 131.4251 - val_accuracy: 0.2174\n","Epoch 50/50\n","7/7 [==============================] - 1s 95ms/step - loss: 13.8016 - accuracy: 0.6537 - val_loss: 134.5216 - val_accuracy: 0.2174\n","2/2 [==============================] - 0s 21ms/step\n","2/2 [==============================] - 0s 22ms/step - loss: 871.4871 - accuracy: 0.9667\n","Test Loss: 871.4871, Test accuracy : 0.9667\n","Epoch 1/50\n","5/5 [==============================] - 1s 102ms/step - loss: 225.5913 - accuracy: 0.3143 - val_loss: 272.2672 - val_accuracy: 0.8750\n","Epoch 2/50\n","5/5 [==============================] - 0s 80ms/step - loss: 141.6331 - accuracy: 0.5571 - val_loss: 88.4818 - val_accuracy: 0.1250\n","Epoch 3/50\n","5/5 [==============================] - 0s 67ms/step - loss: 71.7493 - accuracy: 0.2429 - val_loss: 79.0022 - val_accuracy: 0.3750\n","Epoch 4/50\n","5/5 [==============================] - 0s 65ms/step - loss: 54.5451 - accuracy: 0.5143 - val_loss: 61.2013 - val_accuracy: 0.8750\n","Epoch 5/50\n","5/5 [==============================] - 0s 76ms/step - loss: 47.8101 - accuracy: 0.5714 - val_loss: 48.7433 - val_accuracy: 0.0000e+00\n","Epoch 6/50\n","5/5 [==============================] - 0s 62ms/step - loss: 34.0180 - accuracy: 0.3786 - val_loss: 49.6637 - val_accuracy: 0.0000e+00\n","Epoch 7/50\n","5/5 [==============================] - 0s 75ms/step - loss: 25.6947 - accuracy: 0.4071 - val_loss: 53.6096 - val_accuracy: 0.8750\n","Epoch 8/50\n","5/5 [==============================] - 0s 64ms/step - loss: 22.6168 - accuracy: 0.4786 - val_loss: 49.4139 - val_accuracy: 0.1875\n","Epoch 9/50\n","5/5 [==============================] - 0s 69ms/step - loss: 16.8675 - accuracy: 0.5071 - val_loss: 43.4172 - val_accuracy: 0.8125\n","Epoch 10/50\n","5/5 [==============================] - 0s 62ms/step - loss: 20.4099 - accuracy: 0.4643 - val_loss: 48.9359 - val_accuracy: 0.1250\n","Epoch 11/50\n","5/5 [==============================] - 0s 64ms/step - loss: 15.1093 - accuracy: 0.5643 - val_loss: 47.9403 - val_accuracy: 0.7500\n","Epoch 12/50\n","5/5 [==============================] - 0s 65ms/step - loss: 16.8268 - accuracy: 0.4714 - val_loss: 47.7512 - val_accuracy: 0.7500\n","Epoch 13/50\n","5/5 [==============================] - 0s 68ms/step - loss: 13.4592 - accuracy: 0.6500 - val_loss: 42.8317 - val_accuracy: 0.3750\n","Epoch 14/50\n","5/5 [==============================] - 0s 72ms/step - loss: 8.7431 - accuracy: 0.4857 - val_loss: 46.4633 - val_accuracy: 0.6875\n","Epoch 15/50\n","5/5 [==============================] - 0s 68ms/step - loss: 8.6625 - accuracy: 0.6571 - val_loss: 45.1868 - val_accuracy: 0.4375\n","Epoch 16/50\n","5/5 [==============================] - 0s 63ms/step - loss: 8.3153 - accuracy: 0.5714 - val_loss: 44.6860 - val_accuracy: 0.8125\n","Epoch 17/50\n","5/5 [==============================] - 0s 71ms/step - loss: 6.8068 - accuracy: 0.7000 - val_loss: 48.4041 - val_accuracy: 0.4375\n","Epoch 18/50\n","5/5 [==============================] - 0s 80ms/step - loss: 6.0171 - accuracy: 0.6643 - val_loss: 46.2842 - val_accuracy: 0.6875\n","Epoch 19/50\n","5/5 [==============================] - 0s 71ms/step - loss: 5.2181 - accuracy: 0.7429 - val_loss: 46.1281 - val_accuracy: 0.6250\n","Epoch 20/50\n","5/5 [==============================] - 0s 71ms/step - loss: 4.5215 - accuracy: 0.7071 - val_loss: 45.8856 - val_accuracy: 0.6250\n","Epoch 21/50\n","5/5 [==============================] - 0s 70ms/step - loss: 4.9845 - accuracy: 0.8286 - val_loss: 46.5967 - val_accuracy: 0.5000\n","Epoch 22/50\n","5/5 [==============================] - 0s 62ms/step - loss: 6.1627 - accuracy: 0.8071 - val_loss: 45.0987 - val_accuracy: 0.6875\n","Epoch 23/50\n","5/5 [==============================] - 0s 65ms/step - loss: 18.1414 - accuracy: 0.8214 - val_loss: 46.8618 - val_accuracy: 0.5000\n","Epoch 24/50\n","5/5 [==============================] - 1s 106ms/step - loss: 3.4106 - accuracy: 0.8286 - val_loss: 45.6752 - val_accuracy: 0.7500\n","Epoch 25/50\n","5/5 [==============================] - 0s 99ms/step - loss: 46.1010 - accuracy: 0.8286 - val_loss: 46.7534 - val_accuracy: 0.5625\n","Epoch 26/50\n","5/5 [==============================] - 1s 115ms/step - loss: 15.0136 - accuracy: 0.8429 - val_loss: 46.7522 - val_accuracy: 0.6250\n","Epoch 27/50\n","5/5 [==============================] - 1s 104ms/step - loss: 5.7160 - accuracy: 0.8357 - val_loss: 46.2432 - val_accuracy: 0.6875\n","Epoch 28/50\n","5/5 [==============================] - 1s 116ms/step - loss: 5.5194 - accuracy: 0.8286 - val_loss: 47.4034 - val_accuracy: 0.4375\n","Epoch 29/50\n","5/5 [==============================] - 1s 105ms/step - loss: 4.3170 - accuracy: 0.8071 - val_loss: 46.5290 - val_accuracy: 0.8125\n","Epoch 30/50\n","5/5 [==============================] - 1s 115ms/step - loss: 3.9524 - accuracy: 0.8071 - val_loss: 47.8376 - val_accuracy: 0.3750\n","Epoch 31/50\n","5/5 [==============================] - 1s 126ms/step - loss: 2.8813 - accuracy: 0.8214 - val_loss: 48.5232 - val_accuracy: 0.4375\n","Epoch 32/50\n","5/5 [==============================] - 1s 126ms/step - loss: 10.7254 - accuracy: 0.8000 - val_loss: 46.7386 - val_accuracy: 0.7500\n","Epoch 33/50\n","5/5 [==============================] - 1s 119ms/step - loss: 5.7311 - accuracy: 0.8357 - val_loss: 46.5425 - val_accuracy: 0.5000\n","Epoch 34/50\n","5/5 [==============================] - 1s 114ms/step - loss: 3.5453 - accuracy: 0.8357 - val_loss: 45.8676 - val_accuracy: 0.6250\n","Epoch 35/50\n","5/5 [==============================] - 1s 113ms/step - loss: 2.8304 - accuracy: 0.8643 - val_loss: 48.1347 - val_accuracy: 0.4375\n","Epoch 36/50\n","5/5 [==============================] - 1s 127ms/step - loss: 3.9019 - accuracy: 0.8000 - val_loss: 49.2256 - val_accuracy: 0.4375\n","Epoch 37/50\n","5/5 [==============================] - 1s 105ms/step - loss: 4.1228 - accuracy: 0.6357 - val_loss: 46.2658 - val_accuracy: 0.7500\n","Epoch 38/50\n","5/5 [==============================] - 1s 127ms/step - loss: 5.3410 - accuracy: 0.7429 - val_loss: 47.2329 - val_accuracy: 0.7500\n","Epoch 39/50\n","5/5 [==============================] - 1s 124ms/step - loss: 4.7051 - accuracy: 0.7857 - val_loss: 50.2606 - val_accuracy: 0.3750\n","Epoch 40/50\n","5/5 [==============================] - 1s 110ms/step - loss: 6.1428 - accuracy: 0.7643 - val_loss: 49.5917 - val_accuracy: 0.6250\n","Epoch 41/50\n","5/5 [==============================] - 0s 93ms/step - loss: 4.4705 - accuracy: 0.6714 - val_loss: 48.6725 - val_accuracy: 0.5000\n","Epoch 42/50\n","5/5 [==============================] - 0s 101ms/step - loss: 3.2895 - accuracy: 0.7071 - val_loss: 45.9375 - val_accuracy: 0.6250\n","Epoch 43/50\n","5/5 [==============================] - 1s 117ms/step - loss: 3.3715 - accuracy: 0.6500 - val_loss: 47.0689 - val_accuracy: 0.6875\n","Epoch 44/50\n","5/5 [==============================] - 0s 100ms/step - loss: 2.7232 - accuracy: 0.7286 - val_loss: 47.0264 - val_accuracy: 0.6250\n","Epoch 45/50\n","5/5 [==============================] - 0s 74ms/step - loss: 1.9118 - accuracy: 0.7286 - val_loss: 51.7136 - val_accuracy: 0.3750\n","Epoch 46/50\n","5/5 [==============================] - 0s 71ms/step - loss: 2.3179 - accuracy: 0.7643 - val_loss: 51.0970 - val_accuracy: 0.3750\n","Epoch 47/50\n","5/5 [==============================] - 0s 70ms/step - loss: 3.0418 - accuracy: 0.7357 - val_loss: 47.4648 - val_accuracy: 0.5625\n","Epoch 48/50\n","5/5 [==============================] - 0s 68ms/step - loss: 3.0638 - accuracy: 0.6714 - val_loss: 49.4727 - val_accuracy: 0.7500\n","Epoch 49/50\n","5/5 [==============================] - 0s 71ms/step - loss: 3.1176 - accuracy: 0.5929 - val_loss: 52.6965 - val_accuracy: 0.6875\n","Epoch 50/50\n","5/5 [==============================] - 0s 63ms/step - loss: 4.0554 - accuracy: 0.7643 - val_loss: 46.3747 - val_accuracy: 0.8125\n","2/2 [==============================] - 0s 11ms/step\n","2/2 [==============================] - 0s 14ms/step - loss: 4.5439 - accuracy: 1.0000\n","Test Loss: 4.5439, Test accuracy : 1.0000\n"]}]},{"cell_type":"code","source":["def print_model_summary(loaded_model, tajweed_rule):\n","  print(f'******* Tajweed rule {tajweed_rule} model *******')\n","  loaded_model.summary()\n","  print('\\n')"],"metadata":{"id":"i4FV1w-iDDFU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for rule in tajweed_rules:\n","    model_filename = f'{rule}_tajweed_rule_model'\n","    model_path = os.path.join(export_dir, model_filename)\n","\n","    # Load the saved model\n","    loaded_model = tf.keras.models.load_model(model_path)\n","\n","    print_model_summary(loaded_model, rule)"],"metadata":{"id":"WbRKnuWnLZTc","executionInfo":{"status":"ok","timestamp":1715613872921,"user_tz":-60,"elapsed":4230,"user":{"displayName":"Safa BENABDESSADOK","userId":"13084690164500108428"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"2911fb6e-c016-4d15-da3a-8e15c9bb1ea5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["******* Tajweed rule madd_6_Lazim model *******\n","Model: \"model_10\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_11 (InputLayer)       [(None, 2004, 13)]        0         \n","                                                                 \n"," flatten_10 (Flatten)        (None, 26052)             0         \n","                                                                 \n"," dense_20 (Dense)            (None, 64)                1667392   \n","                                                                 \n"," dense_21 (Dense)            (None, 2)                 130       \n","                                                                 \n","=================================================================\n","Total params: 1667522 (6.36 MB)\n","Trainable params: 1667522 (6.36 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","\n","\n","******* Tajweed rule madd_246 model *******\n","Model: \"model_11\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_12 (InputLayer)       [(None, 3216, 13)]        0         \n","                                                                 \n"," flatten_11 (Flatten)        (None, 41808)             0         \n","                                                                 \n"," dense_22 (Dense)            (None, 64)                2675776   \n","                                                                 \n"," dense_23 (Dense)            (None, 3)                 195       \n","                                                                 \n","=================================================================\n","Total params: 2675971 (10.21 MB)\n","Trainable params: 2675971 (10.21 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","\n","\n","******* Tajweed rule madd_6 model *******\n","Model: \"model_12\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_13 (InputLayer)       [(None, 6267, 13)]        0         \n","                                                                 \n"," flatten_12 (Flatten)        (None, 81471)             0         \n","                                                                 \n"," dense_24 (Dense)            (None, 64)                5214208   \n","                                                                 \n"," dense_25 (Dense)            (None, 6)                 390       \n","                                                                 \n","=================================================================\n","Total params: 5214598 (19.89 MB)\n","Trainable params: 5214598 (19.89 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","\n","\n","******* Tajweed rule madd_2 model *******\n","Model: \"model_13\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_14 (InputLayer)       [(None, 6267, 13)]        0         \n","                                                                 \n"," flatten_13 (Flatten)        (None, 81471)             0         \n","                                                                 \n"," dense_26 (Dense)            (None, 64)                5214208   \n","                                                                 \n"," dense_27 (Dense)            (None, 5)                 325       \n","                                                                 \n","=================================================================\n","Total params: 5214533 (19.89 MB)\n","Trainable params: 5214533 (19.89 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","\n","\n","******* Tajweed rule Ikhfaa model *******\n","Model: \"model_14\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_15 (InputLayer)       [(None, 6267, 13)]        0         \n","                                                                 \n"," flatten_14 (Flatten)        (None, 81471)             0         \n","                                                                 \n"," dense_28 (Dense)            (None, 64)                5214208   \n","                                                                 \n"," dense_29 (Dense)            (None, 9)                 585       \n","                                                                 \n","=================================================================\n","Total params: 5214793 (19.89 MB)\n","Trainable params: 5214793 (19.89 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","\n","\n","******* Tajweed rule Idgham model *******\n","Model: \"model_15\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_16 (InputLayer)       [(None, 6267, 13)]        0         \n","                                                                 \n"," flatten_15 (Flatten)        (None, 81471)             0         \n","                                                                 \n"," dense_30 (Dense)            (None, 64)                5214208   \n","                                                                 \n"," dense_31 (Dense)            (None, 13)                845       \n","                                                                 \n","=================================================================\n","Total params: 5215053 (19.89 MB)\n","Trainable params: 5215053 (19.89 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","\n","\n","******* Tajweed rule tafkhim model *******\n","Model: \"model_16\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_17 (InputLayer)       [(None, 6267, 13)]        0         \n","                                                                 \n"," flatten_16 (Flatten)        (None, 81471)             0         \n","                                                                 \n"," dense_32 (Dense)            (None, 64)                5214208   \n","                                                                 \n"," dense_33 (Dense)            (None, 24)                1560      \n","                                                                 \n","=================================================================\n","Total params: 5215768 (19.90 MB)\n","Trainable params: 5215768 (19.90 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","\n","\n","******* Tajweed rule qalqala model *******\n","Model: \"model_17\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_18 (InputLayer)       [(None, 6267, 13)]        0         \n","                                                                 \n"," flatten_17 (Flatten)        (None, 81471)             0         \n","                                                                 \n"," dense_34 (Dense)            (None, 64)                5214208   \n","                                                                 \n"," dense_35 (Dense)            (None, 6)                 390       \n","                                                                 \n","=================================================================\n","Total params: 5214598 (19.89 MB)\n","Trainable params: 5214598 (19.89 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","\n","\n","******* Tajweed rule imala model *******\n","Model: \"model_18\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_19 (InputLayer)       [(None, 4502, 13)]        0         \n","                                                                 \n"," flatten_18 (Flatten)        (None, 58526)             0         \n","                                                                 \n"," dense_36 (Dense)            (None, 64)                3745728   \n","                                                                 \n"," dense_37 (Dense)            (None, 7)                 455       \n","                                                                 \n","=================================================================\n","Total params: 3746183 (14.29 MB)\n","Trainable params: 3746183 (14.29 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","\n","\n"]}]},{"cell_type":"code","source":["# how data is splitted\n","columns1 = ['tajweed_rule', 'data_of', 'X_train_nb_samples', 'X_test_nb_samples', 'Y_train_nb_samples', 'X_test_nb_samples']\n","splitted_data_info = pd.DataFrame(data=splitted_data_info_np, columns=columns1)\n","\n","# save models information\n","columns2 = ['Model', 'Loss', 'Accuracy', 'Accuracy %', 'Path_to_the_model']\n","models_information = pd.DataFrame(data=models_information_np, columns=columns2)"],"metadata":{"id":"txrZx0e_4Zmw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["splitted_data_info"],"metadata":{"id":"8D4mYXj0Rcqb","executionInfo":{"status":"ok","timestamp":1715613872923,"user_tz":-60,"elapsed":69,"user":{"displayName":"Safa BENABDESSADOK","userId":"13084690164500108428"}},"colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"f4e50495-4827-4242-dcf8-de0deed2f1b7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["    tajweed_rule            data_of X_train_nb_samples X_test_nb_samples  \\\n","0   madd_6_Lazim        Abdul Basit                  1                 1   \n","1   madd_6_Lazim  Yassin Al Jazaery                  1                 1   \n","2   madd_6_Lazim   Ibrahim_Aldosary                  1                 1   \n","3   madd_6_Lazim       all reciters                  3                 3   \n","4       madd_246        Abdul Basit                 62                16   \n","5       madd_246  Yassin Al Jazaery                 62                16   \n","6       madd_246   Ibrahim_Aldosary                 62                16   \n","7       madd_246       all reciters                186                48   \n","8         madd_6        Abdul Basit                 47                12   \n","9         madd_6  Yassin Al Jazaery                 47                12   \n","10        madd_6   Ibrahim_Aldosary                 47                12   \n","11        madd_6       all reciters                141                36   \n","12        madd_2        Abdul Basit                 79                20   \n","13        madd_2  Yassin Al Jazaery                 79                20   \n","14        madd_2   Ibrahim_Aldosary                 79                20   \n","15        madd_2       all reciters                237                60   \n","16        Ikhfaa        Abdul Basit                119                30   \n","17        Ikhfaa  Yassin Al Jazaery                119                30   \n","18        Ikhfaa   Ibrahim_Aldosary                119                30   \n","19        Ikhfaa       all reciters                357                90   \n","20        Idgham        Abdul Basit                114                29   \n","21        Idgham  Yassin Al Jazaery                114                29   \n","22        Idgham   Ibrahim_Aldosary                114                29   \n","23        Idgham       all reciters                342                87   \n","24       tafkhim        Abdul Basit                179                45   \n","25       tafkhim  Yassin Al Jazaery                179                45   \n","26       tafkhim   Ibrahim_Aldosary                179                45   \n","27       tafkhim       all reciters                537               135   \n","28       qalqala        Abdul Basit                 76                20   \n","29       qalqala  Yassin Al Jazaery                 76                20   \n","30       qalqala   Ibrahim_Aldosary                 76                20   \n","31       qalqala       all reciters                228                60   \n","32         imala        Abdul Basit                 52                14   \n","33         imala  Yassin Al Jazaery                 52                14   \n","34         imala   Ibrahim_Aldosary                 52                14   \n","35         imala       all reciters                156                42   \n","\n","   Y_train_nb_samples X_test_nb_samples  \n","0                   1                 1  \n","1                   1                 1  \n","2                   1                 1  \n","3                   3                 3  \n","4                  62                16  \n","5                  62                16  \n","6                  62                16  \n","7                 186                48  \n","8                  47                12  \n","9                  47                12  \n","10                 47                12  \n","11                141                36  \n","12                 79                20  \n","13                 79                20  \n","14                 79                20  \n","15                237                60  \n","16                119                30  \n","17                119                30  \n","18                119                30  \n","19                357                90  \n","20                114                29  \n","21                114                29  \n","22                114                29  \n","23                342                87  \n","24                179                45  \n","25                179                45  \n","26                179                45  \n","27                537               135  \n","28                 76                20  \n","29                 76                20  \n","30                 76                20  \n","31                228                60  \n","32                 52                14  \n","33                 52                14  \n","34                 52                14  \n","35                156                42  "],"text/html":["\n","  <div id=\"df-d0a19074-46f7-4cf8-8a1e-cc36e8f2cd01\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tajweed_rule</th>\n","      <th>data_of</th>\n","      <th>X_train_nb_samples</th>\n","      <th>X_test_nb_samples</th>\n","      <th>Y_train_nb_samples</th>\n","      <th>X_test_nb_samples</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>madd_6_Lazim</td>\n","      <td>Abdul Basit</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>madd_6_Lazim</td>\n","      <td>Yassin Al Jazaery</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>madd_6_Lazim</td>\n","      <td>Ibrahim_Aldosary</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>madd_6_Lazim</td>\n","      <td>all reciters</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>madd_246</td>\n","      <td>Abdul Basit</td>\n","      <td>62</td>\n","      <td>16</td>\n","      <td>62</td>\n","      <td>16</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>madd_246</td>\n","      <td>Yassin Al Jazaery</td>\n","      <td>62</td>\n","      <td>16</td>\n","      <td>62</td>\n","      <td>16</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>madd_246</td>\n","      <td>Ibrahim_Aldosary</td>\n","      <td>62</td>\n","      <td>16</td>\n","      <td>62</td>\n","      <td>16</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>madd_246</td>\n","      <td>all reciters</td>\n","      <td>186</td>\n","      <td>48</td>\n","      <td>186</td>\n","      <td>48</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>madd_6</td>\n","      <td>Abdul Basit</td>\n","      <td>47</td>\n","      <td>12</td>\n","      <td>47</td>\n","      <td>12</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>madd_6</td>\n","      <td>Yassin Al Jazaery</td>\n","      <td>47</td>\n","      <td>12</td>\n","      <td>47</td>\n","      <td>12</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>madd_6</td>\n","      <td>Ibrahim_Aldosary</td>\n","      <td>47</td>\n","      <td>12</td>\n","      <td>47</td>\n","      <td>12</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>madd_6</td>\n","      <td>all reciters</td>\n","      <td>141</td>\n","      <td>36</td>\n","      <td>141</td>\n","      <td>36</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>madd_2</td>\n","      <td>Abdul Basit</td>\n","      <td>79</td>\n","      <td>20</td>\n","      <td>79</td>\n","      <td>20</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>madd_2</td>\n","      <td>Yassin Al Jazaery</td>\n","      <td>79</td>\n","      <td>20</td>\n","      <td>79</td>\n","      <td>20</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>madd_2</td>\n","      <td>Ibrahim_Aldosary</td>\n","      <td>79</td>\n","      <td>20</td>\n","      <td>79</td>\n","      <td>20</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>madd_2</td>\n","      <td>all reciters</td>\n","      <td>237</td>\n","      <td>60</td>\n","      <td>237</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>Ikhfaa</td>\n","      <td>Abdul Basit</td>\n","      <td>119</td>\n","      <td>30</td>\n","      <td>119</td>\n","      <td>30</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>Ikhfaa</td>\n","      <td>Yassin Al Jazaery</td>\n","      <td>119</td>\n","      <td>30</td>\n","      <td>119</td>\n","      <td>30</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>Ikhfaa</td>\n","      <td>Ibrahim_Aldosary</td>\n","      <td>119</td>\n","      <td>30</td>\n","      <td>119</td>\n","      <td>30</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>Ikhfaa</td>\n","      <td>all reciters</td>\n","      <td>357</td>\n","      <td>90</td>\n","      <td>357</td>\n","      <td>90</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>Idgham</td>\n","      <td>Abdul Basit</td>\n","      <td>114</td>\n","      <td>29</td>\n","      <td>114</td>\n","      <td>29</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>Idgham</td>\n","      <td>Yassin Al Jazaery</td>\n","      <td>114</td>\n","      <td>29</td>\n","      <td>114</td>\n","      <td>29</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>Idgham</td>\n","      <td>Ibrahim_Aldosary</td>\n","      <td>114</td>\n","      <td>29</td>\n","      <td>114</td>\n","      <td>29</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>Idgham</td>\n","      <td>all reciters</td>\n","      <td>342</td>\n","      <td>87</td>\n","      <td>342</td>\n","      <td>87</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>tafkhim</td>\n","      <td>Abdul Basit</td>\n","      <td>179</td>\n","      <td>45</td>\n","      <td>179</td>\n","      <td>45</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>tafkhim</td>\n","      <td>Yassin Al Jazaery</td>\n","      <td>179</td>\n","      <td>45</td>\n","      <td>179</td>\n","      <td>45</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>tafkhim</td>\n","      <td>Ibrahim_Aldosary</td>\n","      <td>179</td>\n","      <td>45</td>\n","      <td>179</td>\n","      <td>45</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>tafkhim</td>\n","      <td>all reciters</td>\n","      <td>537</td>\n","      <td>135</td>\n","      <td>537</td>\n","      <td>135</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>qalqala</td>\n","      <td>Abdul Basit</td>\n","      <td>76</td>\n","      <td>20</td>\n","      <td>76</td>\n","      <td>20</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>qalqala</td>\n","      <td>Yassin Al Jazaery</td>\n","      <td>76</td>\n","      <td>20</td>\n","      <td>76</td>\n","      <td>20</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>qalqala</td>\n","      <td>Ibrahim_Aldosary</td>\n","      <td>76</td>\n","      <td>20</td>\n","      <td>76</td>\n","      <td>20</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>qalqala</td>\n","      <td>all reciters</td>\n","      <td>228</td>\n","      <td>60</td>\n","      <td>228</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>imala</td>\n","      <td>Abdul Basit</td>\n","      <td>52</td>\n","      <td>14</td>\n","      <td>52</td>\n","      <td>14</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>imala</td>\n","      <td>Yassin Al Jazaery</td>\n","      <td>52</td>\n","      <td>14</td>\n","      <td>52</td>\n","      <td>14</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>imala</td>\n","      <td>Ibrahim_Aldosary</td>\n","      <td>52</td>\n","      <td>14</td>\n","      <td>52</td>\n","      <td>14</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>imala</td>\n","      <td>all reciters</td>\n","      <td>156</td>\n","      <td>42</td>\n","      <td>156</td>\n","      <td>42</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d0a19074-46f7-4cf8-8a1e-cc36e8f2cd01')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-d0a19074-46f7-4cf8-8a1e-cc36e8f2cd01 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-d0a19074-46f7-4cf8-8a1e-cc36e8f2cd01');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-0ffe623f-b6f1-4ba6-bd2c-f9d7e696c3f4\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0ffe623f-b6f1-4ba6-bd2c-f9d7e696c3f4')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-0ffe623f-b6f1-4ba6-bd2c-f9d7e696c3f4 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"splitted_data_info","summary":"{\n  \"name\": \"splitted_data_info\",\n  \"rows\": 36,\n  \"fields\": [\n    {\n      \"column\": \"tajweed_rule\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"qalqala\",\n          \"madd_246\",\n          \"Idgham\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"data_of\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Yassin Al Jazaery\",\n          \"all reciters\",\n          \"Abdul Basit\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"X_train_nb_samples\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 18,\n        \"samples\": [\n          \"1\",\n          \"3\",\n          \"119\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"X_test_nb_samples\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 16,\n        \"samples\": [\n          \"1\",\n          \"3\",\n          \"36\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Y_train_nb_samples\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 18,\n        \"samples\": [\n          \"1\",\n          \"3\",\n          \"119\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"X_test_nb_samples\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 16,\n        \"samples\": [\n          \"1\",\n          \"3\",\n          \"36\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":37}]},{"cell_type":"code","source":["models_information"],"metadata":{"id":"h3kOCPqVuemS","executionInfo":{"status":"ok","timestamp":1715613872925,"user_tz":-60,"elapsed":34,"user":{"displayName":"Safa BENABDESSADOK","userId":"13084690164500108428"}},"colab":{"base_uri":"https://localhost:8080/","height":331},"outputId":"a6126e0d-e4a0-45b8-966e-16a28115ae85"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                             Model      Loss Accuracy Accuracy %  \\\n","0  madd_6_Lazim_tajweed_rule_model  112.4650   0.6667      66.67   \n","1      madd_246_tajweed_rule_model    1.0305   1.0000     100.00   \n","2        madd_6_tajweed_rule_model  141.7090   0.9444      94.44   \n","3        madd_2_tajweed_rule_model   13.5327   0.9833      98.33   \n","4        Ikhfaa_tajweed_rule_model    0.8618   0.9111      91.11   \n","5        Idgham_tajweed_rule_model    2.7624   0.9770      97.70   \n","6       tafkhim_tajweed_rule_model  208.6373   0.9407      94.07   \n","7       qalqala_tajweed_rule_model  871.4871   0.9667      96.67   \n","8         imala_tajweed_rule_model    4.5439   1.0000     100.00   \n","\n","                                   Path_to_the_model  \n","0  /content/drive/My Drive/M2 GL/PFE/AI_models_v3...  \n","1  /content/drive/My Drive/M2 GL/PFE/AI_models_v3...  \n","2  /content/drive/My Drive/M2 GL/PFE/AI_models_v3...  \n","3  /content/drive/My Drive/M2 GL/PFE/AI_models_v3...  \n","4  /content/drive/My Drive/M2 GL/PFE/AI_models_v3...  \n","5  /content/drive/My Drive/M2 GL/PFE/AI_models_v3...  \n","6  /content/drive/My Drive/M2 GL/PFE/AI_models_v3...  \n","7  /content/drive/My Drive/M2 GL/PFE/AI_models_v3...  \n","8  /content/drive/My Drive/M2 GL/PFE/AI_models_v3...  "],"text/html":["\n","  <div id=\"df-1930cdab-1c9a-4e5c-a51d-bb2d23efd984\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Model</th>\n","      <th>Loss</th>\n","      <th>Accuracy</th>\n","      <th>Accuracy %</th>\n","      <th>Path_to_the_model</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>madd_6_Lazim_tajweed_rule_model</td>\n","      <td>112.4650</td>\n","      <td>0.6667</td>\n","      <td>66.67</td>\n","      <td>/content/drive/My Drive/M2 GL/PFE/AI_models_v3...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>madd_246_tajweed_rule_model</td>\n","      <td>1.0305</td>\n","      <td>1.0000</td>\n","      <td>100.00</td>\n","      <td>/content/drive/My Drive/M2 GL/PFE/AI_models_v3...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>madd_6_tajweed_rule_model</td>\n","      <td>141.7090</td>\n","      <td>0.9444</td>\n","      <td>94.44</td>\n","      <td>/content/drive/My Drive/M2 GL/PFE/AI_models_v3...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>madd_2_tajweed_rule_model</td>\n","      <td>13.5327</td>\n","      <td>0.9833</td>\n","      <td>98.33</td>\n","      <td>/content/drive/My Drive/M2 GL/PFE/AI_models_v3...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Ikhfaa_tajweed_rule_model</td>\n","      <td>0.8618</td>\n","      <td>0.9111</td>\n","      <td>91.11</td>\n","      <td>/content/drive/My Drive/M2 GL/PFE/AI_models_v3...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Idgham_tajweed_rule_model</td>\n","      <td>2.7624</td>\n","      <td>0.9770</td>\n","      <td>97.70</td>\n","      <td>/content/drive/My Drive/M2 GL/PFE/AI_models_v3...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>tafkhim_tajweed_rule_model</td>\n","      <td>208.6373</td>\n","      <td>0.9407</td>\n","      <td>94.07</td>\n","      <td>/content/drive/My Drive/M2 GL/PFE/AI_models_v3...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>qalqala_tajweed_rule_model</td>\n","      <td>871.4871</td>\n","      <td>0.9667</td>\n","      <td>96.67</td>\n","      <td>/content/drive/My Drive/M2 GL/PFE/AI_models_v3...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>imala_tajweed_rule_model</td>\n","      <td>4.5439</td>\n","      <td>1.0000</td>\n","      <td>100.00</td>\n","      <td>/content/drive/My Drive/M2 GL/PFE/AI_models_v3...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1930cdab-1c9a-4e5c-a51d-bb2d23efd984')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-1930cdab-1c9a-4e5c-a51d-bb2d23efd984 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-1930cdab-1c9a-4e5c-a51d-bb2d23efd984');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-6691e56e-03ef-4cb4-842f-bfd5ce72058a\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6691e56e-03ef-4cb4-842f-bfd5ce72058a')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-6691e56e-03ef-4cb4-842f-bfd5ce72058a button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"models_information","summary":"{\n  \"name\": \"models_information\",\n  \"rows\": 9,\n  \"fields\": [\n    {\n      \"column\": \"Model\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"qalqala_tajweed_rule_model\",\n          \"madd_246_tajweed_rule_model\",\n          \"Idgham_tajweed_rule_model\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Loss\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"871.4871\",\n          \"1.0305\",\n          \"2.7624\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accuracy\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"1.0000\",\n          \"0.9770\",\n          \"0.6667\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accuracy %\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"100.00\",\n          \"97.70\",\n          \"66.67\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Path_to_the_model\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"/content/drive/My Drive/M2 GL/PFE/AI_models_v3/qalqala_tajweed_rule_model\",\n          \"/content/drive/My Drive/M2 GL/PFE/AI_models_v3/madd_246_tajweed_rule_model\",\n          \"/content/drive/My Drive/M2 GL/PFE/AI_models_v3/Idgham_tajweed_rule_model\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":38}]}]}